{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 - Gathering and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandas](https://miro.medium.com/max/791/1*e7lYKpF5FJYjNMVPlQgaKg.png)  \n",
    "**Source**: https://analyticsindiamag.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Recap of lesson 2  \n",
    "II. Learning Outcomes  \n",
    "\n",
    "\n",
    "__Outline for the session:__\n",
    "\n",
    "1. Introduction to Pandas  \n",
    "2. DataFrames and Series \n",
    "    - Series\n",
    "    - DataFrames\n",
    "    - Accessing and Selecting Data\n",
    "3. How to get data into Python with pandas\n",
    "    - CSV\n",
    "    - TSV\n",
    "    - Excel\n",
    "    - HTML\n",
    "4. Data Inspection\n",
    "5. Data Cleaning and Preparation for Analysis\n",
    "6. Summary\n",
    "7. References\n",
    "8. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Recap of Lesson 2\n",
    "\n",
    "In the last lesson we covered.\n",
    "\n",
    "1. Lists in Python can be treated as the arrays and matrices that will hold our data for us. The are extremely powerful and versatile data structures and they can be used in almost every aspect of the data analytics cycle that includes analysis.\n",
    "2. NumPy is a library of code built on top of the programming language C. This particular characteristic allows it to communicate with the hardware of our machines and run our computations very, very fast.\n",
    "3. When possible, use broadcasting instead of loop in order to optimise your code and time.\n",
    "4. Generating random data allows us to test models and functions very fast and numpy can help us very well with this. It has functions such as `np.ones`, `np.random.random`, `np.linspace`, and many more.\n",
    "5. Masking is a type of filtering method that allows us to have a closer look at our data. It is, in a way, an equivalent way of constructing if-else statements.\n",
    "6. List comprehensions are a type of for loop that gives us the ability to generate a list from repeated commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Learning Outcomes\n",
    "\n",
    "1. Introduce you to how to create and load datasets in Python using the pandas library\n",
    "2. Learn how to manipulate datasets and interact with their objects\n",
    "3. Learn how to clean and prepare data for analysis\n",
    "4. Understand why data preparation is one of the most important steps in the data analytics cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to pandas\n",
    "\n",
    "![pandas](https://i.redd.it/c6h7rok9c2v31.jpg)  \n",
    "**Source**: https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas](https://pandas.pydata.org/) is a Python library originally developed with the goal of making data manipulation and analysis in Python easier. The library was created by Wes McKinney, and it was first released in 2010. It has been designed to work with tabular data that does not necessarily, unlike NumPy, has to have the same kind of data type in a column or array. pandas gives you, in a way, the same capabilities you would get when working with data in tools such as Microsoft Excel or Google Spreadsheets, but with the added benefit of allowing you to use more data.\n",
    "\n",
    "In essence, pandas allows you to finely-tune most of the computations you would like to apply to your data.\n",
    "\n",
    "The pandas library is also mostly built on NumPy, this means that a lot of the functionalities that you learned in the previous lesson will transfer seamlessly to this lesson and this new tool we will explore. What you will find in pandas is, the ability to control your NumPy arrays as if you using a spreadsheet.\n",
    "\n",
    "Some of pandas main characteristics are:\n",
    "\n",
    "- Straightforward and convinient way for loading and saving datasets of and into different formats, respectively\n",
    "- Swiss army knife for data cleaning\n",
    "- Allows for the broadcasting of operation, hence, if you can avoid loops... (ðŸ³, up to you)\n",
    "- Allows for different data types and structures inside its two main data structures, Series and DataFrames\n",
    "\n",
    "pandas, like NumPy, also has a industry standard alias that we will be using throughout the course. This library is usually imported as `pd`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "Just like NumPy has the very efficient data structure called `ndarray`'s, pandas, as NumPy's child, has its own structures called `DataFrame`s (the equivalent of a NumPy matrix), and `Series` (the equivalent of a NumPy array). We will cover these two structures next.\n",
    "\n",
    "**Warning:** It is possible that the control boost you will feel as you begin to learn how to use pandas to clean, manipulate, and analyse data, will prevent you from going back to using the tools you have been using in the past (e.g. Excel, Google Sheets, regular calculators, etc.). ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DataFrames and Series in pandas\n",
    "\n",
    "![pandas](https://media.giphy.com/media/txsJLp7Z8zAic/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to import data into Python from outside sources, we'll walk over how to transform existing data (i.e., data we will come up with it), into the two main data structures of pandas, `DataFrame`s and `Series`. We will do so through several different avenues, so let's first talk about what are `DataFrame`s and `Series`.\n",
    "\n",
    "A pandas `Series` is the equivalent of a column in a pandas `DataFrame`, a one-dimensional numpy array, or a column or row in Excel. In fact, since pandas derives most of its functionalities from NumPy, you can tranform the type of a Series by applying the attribute `.values` on to it, and pandas will return a `numpy.ndarray`. A Series has most of the functionalities you will see in a `DataFrame` and they can be combined to form a complete one as well. Let's switch our attention to `DataFrame`s now.\n",
    "\n",
    "A `DataFrame` is a data structure particular to pandas that allows us to work with data in a tabular format. One of the best analogies for pandas' `DataFrame`s is that they allow us to manipulate data as if it were inside a spreadsheet. You can also think of a pandas DataFrames as a NumPy matrix with more flexibility. Some characteristics of `DataFrame`s are:\n",
    "\n",
    "- they have a two-dimesional matrix shape (but can also handle more dimensions)\n",
    "- their rows and columns are clearly defined with a visible indexes and names, respectivelt, when displayed\n",
    "- indexes are explicit and visible upon immediate inspection of the dataframe\n",
    "- they have a plethora of functionalities for reshaping, cleaning, and munging the data\n",
    "- Indexes can be strings, dates, numbers, etc.\n",
    "\n",
    "Let's first start by importing `pandas` with its industry alias, `pd`, and then checking the version we have installed in our machines.\n",
    "\n",
    "**Note:** At the time of writing, the latest version of pandas is 1.0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some fake data first and reshape it into a pandas `Series`. We will do so in the following ways:\n",
    "- with lists\n",
    "- with NumPy arrays\n",
    "- and with dictionary objects with lists or tuples\n",
    "\n",
    "Say we have data for pizzas purchased at different stores and that we would like to manipulate these data using a pandas `Series` and assign it to a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be your fake pizza data representing amount of pizzas purchased\n",
    "[2, 1, 6, 5, 1, 4, 2, 6, 2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Series we use the `pd.Series(data= , name=)` method, pass our data through the `data=` parameter and give it a name using the `name=` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be your first pandas Series\n",
    "first_series = pd.Series(data=[2, 1, 6, 5, 1, 4, 2, 6, 2, 1], name='pizzas')\n",
    "first_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use NumPy arrays for the data we pass into our Series. As noted earlier, while using pandas we are essentially using NumPy structures in an indirect way.\n",
    "\n",
    "Another neat functionality of Series is that they are not bound to only having numerical indexes. As shown in the example below, we can add our own indexes to a pandas Series and they can also be strings or dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_series = pd.Series(data=np.arange(8), \n",
    "                          index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], \n",
    "                          name='random_data')\n",
    "second_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we spent quite some time on NumPy, one of the reasons for this is that pandas is a module built on top of NumPy, so a lot of the methods, and the slicing and dicing techniques you've already learned, will be applicable to pandas data structures too. For example, broadcasting operations over an entire array, instead of using a loop, are perfectly doable doable operations with pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 5 to every element in our second_series\n",
    "second_series + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise every element to the power of 3\n",
    "first_series ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind though, that when you do broadcasting on a pandas object, the change won't happend inplace and you would have to assign the changed object to a new variable or the same one to keep it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Series did not keep the changes\n",
    "print(first_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the Series will keep the changes\n",
    "first_series = first_series ** 3\n",
    "print(first_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning again that if you would like to access the NumPy structure underneath a pandas Series, you can do so by calling the attribute `.values` on the pandas Series. That way what you get back is a `numpy.ndarray`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a dictionary of key-value pairs to create our pandas Series. The only caveat is that since key-value pair structures can contain a lot of data, we have to explicitely call out the data we want in the rows by using the name of the key on the dictionary. If we did not select the key or keys for the data we want, it would assign them to the index of the Series and the values as the elements of those keys. The result would not be any better that using the regular dictionary itself unless, of course, there is a need for this kind of structure.\n",
    "\n",
    "Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizzas = {'pizzas': [2, 1, 6, 5, 1, 4, 2, 6, 2, 1]}\n",
    "# Not good, one index only\n",
    "third_series = pd.Series(data=pizzas['pizzas'])\n",
    "third_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try The Following\n",
    "\n",
    "Try using the above method without calling the key of the dictionary and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also think of a DataFrame as a collection of Series with the difference being that all of the elements in those Series will share the same index once they are in the DataFrame.\n",
    "\n",
    "Another distinction between the two is that you can have a DataFrame of only one column, but you cannot have a Series of more than one (or at least you shouldn't since that is what the DataFrame is for).\n",
    "\n",
    "Let's now create some fake data and reshape it into a pandas `DataFrame` object. We will do so in the following ways:\n",
    "- a dictionary object with lists and tuples\n",
    "- lists and/or tuples\n",
    "- NumPy arrays\n",
    "- multiple pandas Series\n",
    "\n",
    "One of the fastest and more common ways to construct a DataFrame is by passing in a Python dictionary to the `data=` parameter in the `pd.DataFrame()` method. Doing this with dictionaries can save us time with having to name each one of the columns in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of fake pizza data\n",
    "data_le_pizza = {\n",
    "    'pizzas': [2, 1, 6, 5, 1, 4, 2, 6, 2, 1], # some fake pizzas purchased\n",
    "    'price_pizza': (20, 16, 18, 21, 22, 27, 30, 21, 22, 17), # some fake prices per pizza \n",
    "    'pizzeria_location': ['Sydney', 'Sydney', 'Seville', 'Perth', 'Perth', 'Melbourne',\n",
    "                          'Sydney', 'Seville', 'Melbourne', 'Perth']\n",
    "}\n",
    "\n",
    "data_le_pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data in the dictionary\n",
    "data_le_pizza['pizzas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_le_pizza['price_pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_le_pizza['pizzeria_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza = pd.DataFrame(data=data_le_pizza)\n",
    "df_la_pizza.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our new object, the pandas DataFrame, resembles the way we would see data in a spreadsheet.\n",
    "\n",
    "You can access the data inside your new DataFrame by calling the names of your columns as attributes or as a key in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the pizzas column as a method\n",
    "df_la_pizza.pizzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the pizzas variable as the key of a dictionary\n",
    "df_la_pizza['pizzas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can broadcast operations to an entire column the same way you did with the Series in this lesson and the NumPy data structures in the previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza['pizzas'] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add the values to an entire DataFrame or subsection of it, although this might not be possible or desirable if all of the columns contain different data types. For example the following code will give you an error but the subsequent one, the group of numerical columns, won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza[['pizzas', 'price_pizza']] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have several useful attributes such as `.index` and `.columns` that allows us to retrieve valuable information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows us the start, stop, and step of our DataFrame's index, a.k.a. the range of the index\n",
    "df_la_pizza.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the names of the columns we have in our DataFrame\n",
    "df_la_pizza.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add new columns by passing in the name of the new column as a key just like in a dictionary, and the corresponding values as an operation after an assignment identical to that used when creating variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza['new_pizzas'] = df_la_pizza['pizzas'] * 3.5\n",
    "df_la_pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas also gives us the option of naming the set of columns we have as well as the index column of our DataFrame. We can do this by calling the sub-attribute `.name` on the `.columns` and `.index` attributes of our DataFrame. Let's name our columns array `pizza_attr` for pizza attributes, and let's name our index array `numbers` to see this functionality of pandas in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la_pizza.columns.name = 'pizza_attr'\n",
    "df_la_pizza.index.name = 'numbers'\n",
    "df_la_pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the new element assignment happened in place and now our DataFrame displays even more information than before.\n",
    "\n",
    "If we wanted to get rid of a column we don't need or want anymore, we can use `del` call of Python, just like we saw in the chapter of lists, arrays, and matrices in lesson 2.\n",
    "\n",
    "For illustration purposes, let's delete the `new_pizzas` column we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_la_pizza['new_pizzas']\n",
    "df_la_pizza # notice that the column is now gone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how to convert a list of lists and tuples into a pandas DataFrame. We will first create a list called `la_pizzas` with lists and tuples, and then pass this matrix into our DataFrame constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_pizzas = [[2, 20, 'Sydney'],\n",
    "            [1, 16, 'Sydney'],\n",
    "            (6, 18, 'Seville'),\n",
    "            [5, 21, 'Perth'],\n",
    "            [1, 22, 'Perth'],\n",
    "            (4, 27, 'Melbourne'),\n",
    "            [2, 30, 'Sydney'],\n",
    "            (6, 21, 'Seville'),\n",
    "            [2, 22, 'Melbourne'],\n",
    "            [1, 17, 'Perth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = pd.DataFrame(data=la_pizzas, \n",
    "                      columns=['pizzas', 'price_pizza', 'pizzeria_location'])\n",
    "df_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add completely new lists to our existing DataFrame, and pandas will match the index of each element in our new list with the index of each element in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pizza_code = list(range(20, 40, 2))\n",
    "new_pizza_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['new_pizza_code'] = new_pizza_code\n",
    "df_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the length of a list does not match that of our DataFrame, pandas will throw an error at us for the mismatched lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_list = list(range(40, 55, 2))\n",
    "df_one['another_list'] = another_list\n",
    "df_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how can we use numpy arrays and matrices to create a DataFrame.\n",
    "\n",
    "We will begin with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create our la_pizza numpy matrix\n",
    "\n",
    "la_pizza_np = np.array([[2, 20, 'Sydney'],\n",
    "                        [1, 16, 'Sydney'],\n",
    "                        [6, 18, 'Seville'],\n",
    "                        [5, 21, 'Perth'],\n",
    "                        [1, 22, 'Perth'],\n",
    "                        [4, 27, 'Melbourne'],\n",
    "                        [2, 30, 'Sydney'],\n",
    "                        [6, 21, 'Seville'],\n",
    "                        [2, 22, 'Melbourne'],\n",
    "                        [1, 17, 'Perth']])\n",
    "\n",
    "la_pizza_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we pass in the matrix to the and provide a list of names for the columns\n",
    "\n",
    "df_np_pizza = pd.DataFrame(la_pizza_np, columns=['pizzas', 'price_pizza', 'pizzeria_location'])\n",
    "df_np_pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the shape of our new dataframe\n",
    "\n",
    "df_np_pizza.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that creating a matrix where the row arrays represent three different columns is not the same as creating three array thats represent three rows and and 10 columns. Our intuition might betray us in this sence.\n",
    "\n",
    "Let's look at an example with fake weather data where we pass in three arrays to a NumPy array that should represent the same DataFrame as the one above, but with different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_np = np.random.randint(10, 45, 10)\n",
    "weather_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Sydney', 'Sydney', 'Seville', 'Perth', 'Perth', \n",
    "          'Melbourne', 'Sydney', 'Seville', 'Melbourne', 'Perth']\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = np.random.randint(10, 30, 10)\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather = np.array([weather_np,\n",
    "                         cities,\n",
    "                         days])\n",
    "data_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of our new matrix. What do you think will happen when we pass to our DataFrame constructor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=data_weather, columns=['weather', 'cities', 'days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part of using NumPy arrays lies in the fact that the arrays are interpreted as horizontal arrays, meaning, we would have 10 columns and 3 rows if we were to use our array with its current shape. You probably noticed this already by running the code above.\n",
    "\n",
    "The solution is to transpose our matrix and shift the columns to the rows and the rows to the columns. NumPy provides a very nice way for doing this. By adding the method `.T` at the end of any array or matrix you can transpose it into a different shape.\n",
    "\n",
    "Let's see what this looks like and then use it to create our new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same list as before with the pizzasðŸ˜Ž\n",
    "\n",
    "data_weather.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=data_weather.T, columns=['weather', 'cities', 'days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, imagine we had several pandas Series representing different values but with similar indexes. If we wanted to combine all of these into a single DataFrame to use them in combination, we could do so with `pd.concat([Series1, Series2, Series3])`, or with `pd.DataFrame(data=dictionary)` where the keys of the dictionary would represent the variables (e.g. the names of the columns) in the DataFrame, and the values would be the pandas Series (e.g. the elements of the columns) you will be using in your DataFrame.\n",
    "\n",
    "One important thing to keep in mind is that, just like with the `np.concatenate` we saw in the last lesson, you will need to pick an axis when using this method.\n",
    "\n",
    "**Note:** pandas will try to match the indexes of your multiple Series when combining their elements, but, if the indexes do not match, it will add an `np.nan` (Not a Number) at that place to show that that particular element does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_one = pd.Series(np.random.randint(0, 20, 20), name='random_nums')\n",
    "series_two = pd.Series(list(range(20, 60, 2)), name=\"two_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_of_series = pd.concat([series_one, series_two], axis=1)\n",
    "df_of_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same approach as above but with dictionaries of Series's\n",
    "\n",
    "dict_of_series = {\n",
    "    'random_nums': pd.Series(np.random.randint(0, 20, 20), name='random_nums'),\n",
    "    'two_steps': pd.Series(list(range(20, 60, 2)), name=\"two_steps\")\n",
    "}\n",
    "dict_of_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dict_series = pd.DataFrame(dict_of_series)\n",
    "df_dict_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Set of Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Canvas and download the notebook titles **\"session_pandas_exercises.ipynb\"**. Complete the exercises under **First Set of Exercises**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Accessing and Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access and select data in a pandas DataFrame we can use the same tools we learned in lesson 2, \\[start:stop:step, start:stop:step\\] for rows and columns, fancy indexing, and masking for n-dimensional arrays. pandas also provides us with two additional tools for accessing data inside a DataFrame `df.loc[]` and `df.iloc[]`.\n",
    "\n",
    "- `df.loc[]` helps us select data in the same manner as with NumPy arrays except that we need to select the columns by their names and not by their numbers.\n",
    "- `df.iloc[]` allows to select rows and columns by numbers. For example, if I have the columns `[weather, cities, days]`, I could select weather with index 0, cities with index 1, and days with index 2, just like with NumPy.\n",
    "\n",
    "Let's look at the regular way first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.DataFrame(data=data_weather.T, columns=['weather', 'cities', 'days'])\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[df_weather['cities'] == 'Perth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.loc[df_weather['cities'] == 'Sydney']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, quick and dirty approach, but if we wanted to get more granular with how we select our data, we would have to resort to the additional functionality of `.iloc[]` or `.loc[]` since NumPy slicing for rows is not as straightforward as with pandas.\n",
    "\n",
    "It is important to note that `.iloc[]` and `.loc[]` are both inclusive of the end point of a slice. Meaning, `df.iloc[:10]` would actually select the element at index 10 as well. The same would apply to the columns.\n",
    "\n",
    "Let's look at pandas methods for slicing and dicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first 7 rows of the days column\n",
    "\n",
    "df_weather.loc[0:7, 'days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first 5 rows of the days and weather columns\n",
    "\n",
    "df_weather.loc[0:5, ['weather', 'days']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before but with iloc now and integers\n",
    "\n",
    "df_weather.iloc[0:7, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.iloc[0:5, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.iloc[0:-2, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these methods, `.iloc[]` and `.loc[]` will become extremely useful as we move along the course and our data analytics journey. A good tip for remembering the differences between the two is to always think of integers when you see the i in `.iloc[]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting Data into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data in Python you will encounter datasets coming in all shapes and formats, so it is crucial to understand how to deal with them in order to efficiently work with data. We will be covering the following 4 formats in this section (yes, there are only 4 here ðŸ˜):\n",
    "\n",
    "- CSV --> Comma Separated Values --> `pd.read_csv(file, sep=',')`\n",
    "- TSV --> Tab Separated Values --> `pd.read_csv(file, sep=' ')`\n",
    "- Excel --> Microsoft Excel format (.xlsx) --> `pd.read_excel()`\n",
    "- JSON --> JavaScript Object Notation --> `pd.read_json()`\n",
    "- HTML --> Hypertext Markup Language --> `pd.read_html()`\n",
    "\n",
    "For this part of the lesson, we will be using some real world datasets that you can find in Canvas under this lesson's module. Please download them and add them to a new folder inside this course's folder. We will only load them in this lesson but from next lesson onwards, we will be analysing and visualising some of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text files are extremely common among organisations, and hence, they will form a big part of the files you will encounter in your daily work. More specifically, files such as comma and tab separated values, along with other text files that used different delimiters, might amount to 75% (if not more) of the files that you will see at work.\n",
    "\n",
    "These two formats are very useful for saving and distributing small and large datasets in a tabular format, especially because they also take less memory space.\n",
    "\n",
    "You can identify both kinds of files by looking at the suffix part in the name of a file, comma separated values will end in `.csv` while tab separated values will end with `.tsv`.\n",
    "\n",
    "What makes these two files so similar is that they are both separated by what is commonly known as a delimiter. If you have a CSV or TSV file, try opening them in a plain text editor application and notice what comes up.\n",
    "\n",
    "![csv](pictures/csv_file.png)\n",
    "\n",
    "Notice that in the example above, every value is separated by a comma and the column headers can be found at the very top of the file. When we save files as TSV, words with spaces in them will be wrapped around quotation marks to differentiate the spaces of the delimiter from the spaces in the data.\n",
    "\n",
    "Lastly, let's talk about how pandas handles these types of files. pandas uses the general method `pd.read_csv()`, and this method, at the time of writing, has over 50 parameters that allows us to customise the way in which we can read text files. One of the most important parameters is the `sep=`, which allows us to definer a delimiter. The default option is the comma, and to use a tab delimiter we can choose `sep=\"\\t\"`.\n",
    "\n",
    "The following parameters are some of the most useful ones not only for reading text files specifically, but also for many others of the pandas methods for reading data. Please visit the pandas documentation for more info.\n",
    "\n",
    "- `header=` --> tells pandas whether the first column contains the headers of the dataframe or not.\n",
    "- `names=[list, of, column, names]` --> allows us to explicitly name the columns of a dataframe in the order in which they are read.\n",
    "- `parse_dates=` --> gives pandas permision to look for what might look like date data and it will assign it the appropriate date data type format.\n",
    "- `index_col=` --> allows us to assign multiple indexes to a dataframe. Think of this as a pivot table in Excel with multiple layers.\n",
    "- `skiprows=\\[1, 2, 3, 4\\]` --> tells pandas which rows we want to skip.\n",
    "- `na_values=` --> takes in a list of values that might be NULL or NaN, which stands for not a number. Both of these represent missing values in Python and many other programming languages.\n",
    "- `encoding=` --> data might coming in from a variety of sources might come with different encodings, e.g. 'UTF-8', and this parameter helps us specify which one we need.\n",
    "- `nrows=4` --> how many rows do you want to read from a file. Very useful tool for reading large files.\n",
    "\n",
    "Let's use the Air Quality Monitoring Dataset and let's read in the CSV first and then the TSV one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the file names inside the folder containing your data you can use the following command\n",
    "\n",
    "!ls ../datasets/files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the folder where the data lives, followed the name of the data. Once you load the dataset and assign it to a variable, you can see the first 5 lines of it with the method `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first argument is the folder where the data lives and the name of the data\n",
    "\n",
    "df_csv = pd.read_csv('../datasets/files/seek_australia.csv')\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in TSV files, all we need to do is to pass in the `sep=` parameter and provide pandas with a specific delimiter on how to split the data by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv = pd.read_csv('../datasets/files/Air_Quality_Monitoring_Data.tsv', sep='\\t')\n",
    "df_tsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another method in pandas that uses the Tab Separated Values delimiter `\"\\t\"` as its default delimiter, and that is the `pd.read_table()` method. You should use whichever you prefer, especially as most of the options of one can be found in the other. This means that by indicating the `sep=','` with a comma, you can obtain the same result as with the `pd.read_csv()` and read in Comma Separated Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.read_table('../datasets/files/occupational_licences.tsv')\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel files are very common as well, especially if members of your team use it for the analysis. This would mean that you would have to constantly read Excel files at work. Fortunately, pandas provides a nice method to read in excel files, and in particular, different and/or specific sheets inside of it.\n",
    "\n",
    "The pandas method, `pd.read_excel()`, just like read_csv, provides a plethora of options that you can choose from, should the complexity of your file requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel(\"../datasets/files/supermarket_demo.xlsx\", sheet_name='supermarket_demo', parse_dates=True)\n",
    "df_excel.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 HTML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has the ability to read HTML tables from a website with the method `pd.read_html()` but it is not as complete a tool as Scrapy or BeautifulSoup. These last two libraries are very powerful web scraping tools that you are more than encouraged to explore on your own. Intermediate to complex web scraping requires a fair amount of knowledge on how the structure of a website works. With a few hours of studying, or in the next couple of minutes, you might be well on your way to scraping your own data.\n",
    "\n",
    "We will be scraping the following website --> https://www.fdic.gov/bank/individual/failed/banklist.html\n",
    "\n",
    "Before we explore pandas method for web scraping, let's quickly define it:\n",
    "\n",
    "> **Web Scraping** refers to extracting data, structured or unstructured, from websites and making it useful for a variety of purposes, such as marketing analysis. Companies in the marketing arena use web scraping to colect comments about their products. Others, like Google, scrape the entire internet to rank websites given a criterion or search query. While web scraping might be limited in scope to a single website, like what a marketer might do, **web crawling** is the art of crawling over many different and/or nested websites on one try, or repeadately over time, like what Google does.\n",
    "\n",
    "**Note:** pandas `pd.read_html()` method captures the tables in a list object, which means that you would have to first assign the list to a variable and then dump it into a dataframe object. Also, you might need the following libraries if the operation below does not run.\n",
    "\n",
    "- `conda install lxml`\n",
    "- `pip install beautifulsoup4 html5lib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_html('https://www.fdic.gov/bank/individual/failed/banklist.html')\n",
    "df_html = pd.DataFrame(data[0])\n",
    "df_html.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data detectives we are, our first task after we load the data into Python will always be to inspect it to see if we can spot any inconsistencies that might need to be dealt with immendiately. In order to accomplish this, pandas provides us with a good set of tools (`.methods()`) that will become our best firends as we move along our data analytics journey. Let's describe each one of these methods and look at what each one of them does with one or all of the datasets we have loaded into memory in the previous steps.\n",
    "\n",
    "- `df.head()` --> shows the first 5 rows of a DataFrame or Series\n",
    "- `df.tail()` --> shows the last 5 rows of a DataFrame or Series\n",
    "- `df.info()` --> provides information about the DataFrame or Series\n",
    "- `df.describe()` --> provides descriptive statistics of the numerical variables in a DataFrame\n",
    "- `df.isna()` --> returns True for every element that is NaN and False for every element that isn't\n",
    "- `df.notna()` --> does the opposite of `.isna()`\n",
    "\n",
    "Let's see all of these in action now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second set of Exercises\n",
    "\n",
    "Go to the notebook titled \"session_pandas_exercises.ipynb\" and complete the exercises in **Second set of Exercises**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Cleaning and Preparation for Analysis\n",
    "\n",
    "![pandas_tools](https://i.chzbgr.com/full/1898496256/h42C0CC42/panda-cleaning-instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the the following tools for our data cleaning, preparation and analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import datetime\n",
    "import altair as alt\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using contains biometric data and it comes from a Garmin watch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ramonprz01/codevelop-march-2020/master/data/activities_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Make your columns work for you\n",
    "\n",
    "Let's start by having a look at the columns of our dataframe. We can accomplish this by calling the attribute `.columns` on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the variables in any dataset can often come with spaces and some uppercase letters that might make it harder to interact with them individually. It is useful to convert these to lower case and substitute the space with an underscore, so let's do that next.\n",
    "\n",
    "To access an array that comes from a dataframe and has strings in it, we need to tell python that we are accessing strings with the `.str` attribute, and then use a the method `.replace()` with the first argument being what we want to replace and the second what we want to replace it with. In our case, we want to replace the space `' '` with and underscore `'_'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our columns to make sure we changed all of our variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Clean the Data\n",
    "\n",
    "If you noticed earlier when you used the `.tail()` method on the dataframe, we have values that are not useful for any type of calcualtion. These are `\"--\"` and we should change them to the more conventional `np.nan` values, which is consider \"Not a Number.\"\n",
    "\n",
    "We can accomplish this with the same `.replace()` method we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('--', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Get rid of variables that you won't be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can often come with variables that don't carry any useful information or are simply not useful for our purposes, in this case, the following variables represent both instances just described.\n",
    "\n",
    "We can drop variables and/or rows in pandas using the `.drop()` method and passing a list of columns or indexes we want to get rid of. The `axis=` parameter allows us to specify whether the change will happen in the columns (1) or in the rows (0). Lastly, if we would like our method to be evaluated inplace, we can do so by fixing the parameter `inplace=` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['avg_vertical_ratio', 'avg_vertical_oscillation',\n",
    "         'training_stress_scoreÂ®', 'grit', 'flow', 'favorite',\n",
    "         'bottom_time', 'surface_interval', 'best_lap_time', \n",
    "         'max_temp', 'decompression'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['time'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['calories'] = pd.to_numeric(df['calories'])\n",
    "df['aerobic_te'] = pd.to_numeric(df['aerobic_te'])\n",
    "df['avg_run_cadence'] = pd.to_numeric(df['avg_run_cadence'])\n",
    "df['max_run_cadence'] = pd.to_numeric(df['max_run_cadence'])\n",
    "df['number_of_runs'] = pd.to_numeric(df['number_of_runs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['week'] = df['date'].dt.week\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['time_exercise'] = df['date'].dt.time\n",
    "df['date_exercise'] = df['date'].dt.date\n",
    "df['day_of_week'] = df['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day = []\n",
    "\n",
    "for i in df['time_exercise']:\n",
    "    if i > datetime.time(5, 59, 59) and i < datetime.time(12, 0, 0):\n",
    "        time_of_day.append('morning')\n",
    "    elif i > datetime.time(11, 59, 59) and i < datetime.time(18, 0, 0):\n",
    "        time_of_day.append('afternoon')\n",
    "    else:\n",
    "        time_of_day.append('night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_day'] = time_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_or_end = []\n",
    "\n",
    "for day in df['weekday']:\n",
    "    if day >= 5:\n",
    "        week_or_end.append('weekend')\n",
    "    else:\n",
    "        week_or_end.append('week_day')\n",
    "\n",
    "week_or_end[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_or_end'] = week_or_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Analysing and Visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(\n",
    "    index=['week_or_end', 'time_day'],\n",
    "    columns='year',\n",
    "    values='calories',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(\n",
    "    index=['week_or_end', 'time_day'],\n",
    "    columns='year',\n",
    "    values='calories',\n",
    "    aggfunc=['mean', 'median']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group = df.groupby(['time_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['activity_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['week'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['activity_type'].value_counts(normalize=True)['night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['calories'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['calories'].agg(['median', 'mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['calories'] == max(df['calories']))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = today - df[mask]['date_exercise']\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((difference.iloc[0].total_seconds() / 60) / 60) / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_day_group['distance'].plot(kind='hist', title=\"Histogram of Distance\",\n",
    "                                bins=25, alpha=0.7, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['year'] == 2018, 'distance'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['distance', 'calories']].plot.hexbin(x='calories', y='distance', gridsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['distance'], bins=40, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['avg_hr'], bins=30, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df, kind='hex');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='distance', y='calories', data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['distance', 'calories', 'avg_hr', 'max_hr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df[['avg_hr', 'max_hr', 'calories', 'distance', 'title']]).mark_circle(size=60).encode(\n",
    "    x='distance',\n",
    "    y='calories',\n",
    "    color='title',\n",
    "    tooltip=['avg_hr', 'max_hr', 'calories', 'distance', 'title']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df[['avg_hr', 'max_hr', 'calories', 'distance', 'title', 'time_day']]).mark_circle(size=60).encode(\n",
    "    x='distance',\n",
    "    y='calories',\n",
    "    color='time_day',\n",
    "    tooltip=['avg_hr', 'max_hr', 'calories', 'distance', 'title', 'time_day']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we have covered pandas in great lenght, and still, we have not even began to scratch the surface of what this powerful tool can do. Some keypoints to take away:\n",
    "\n",
    "- pandas provides two fantastic data structures for data analysis, the DataFrame and the Series\n",
    "- We can slice and dice these data structures to our hearts content but we have to keep in mind the inconsistencies that we might find in different datasets\n",
    "- We should always begin by exploring our dataset immediately after loading the data. pandas provides methods such as info, describe, and isna that work very well and allow us to explore the data\n",
    "- Don't try to learn all the tools inside pandas but rather explore the ones you need as the need arises, or, explore them slowly and build an intuition for them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweigart, Al. _Automate the Boring Stuff with Python: Practical Programming for Total Beginners_. No Starch Press, 2020.\n",
    "\n",
    "VanderPlas, Jake. _A Whirlwind Tour of Python_. O'Reilly, 2016.\n",
    "\n",
    "VanderPlas, Jake. _Python Data Science Handbook_. O'Reilly, 2017.\n",
    "\n",
    "McKinney, Wes. _Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython_. OReilly, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would really appreciate it if you could please provide us with your feedback from this session by filling a couple of question.\n",
    "\n",
    "> ## [Survey](https://docs.google.com/forms/d/e/1FAIpQLSfPGuCaT4b5QQcEKXfY_X999gxZ_CQ5arJeUnOu0r-MRI8xeg/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
