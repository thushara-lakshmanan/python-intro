{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - Statistics Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stats2](https://cdns.tblsft.com/sites/default/files/pages/1_napolean_minard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Recap of lesson 5  \n",
    "II. Learning Outcomes  \n",
    "\n",
    "\n",
    "__Outline for the session:__\n",
    "\n",
    "1. Introduction to Statistical Inference\n",
    "2. Data\n",
    "3. Intro to SciPy\n",
    "4. Distributions\n",
    "5. Hypothesis Testing\n",
    "6. Summary  \n",
    "7. References  \n",
    "8. Feedback  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Recap of Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 5, we covered the basics of Statistics as well as two of its major fields, which deal with collecting and describing data. We touched on:\n",
    "\n",
    "- The shape and form of how data might be collected, structured and unstructured\n",
    "- Different approaches for collecting data through sampling, observation and experimentation\n",
    "- The characteristics of measures of central tendency and measures of variability and what each describes\n",
    "- In-depth descriptive statistics and how to calculate these with plain Python\n",
    "- We also covered a variety of cross-tabulation methods with `pivot_table()` and the `.groupby()` functions\n",
    "    - The best way to think of `.groupby()` is by stating: _for each \"category or group\" show me \"x\" aggregation by column(s)_.\n",
    "    - The `pivot_table()` function provides us with more detailed control over the way we cross-tabulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the lesson you will have learned:\n",
    "\n",
    "1. What is Statistics Inference and what role does it play in the data analytics cycle.\n",
    "2. What is a probability density function and how do we represent it.\n",
    "3. What is hypothesis testing, and which method to use in a variety of circumstances.\n",
    "4. How to test and evaluate different hypotheses tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inf](https://user-images.githubusercontent.com/35218826/37026890-1e2bcfe0-2173-11e8-9b06-3db329d5f477.png)\n",
    "\n",
    "Statistical inference is one of the divisions of statistics that deals with what cannot (or has not) been observed in the data. It is also the side of statistics used to make predictions about what might happen in the future based on what has happen in the past. In other words, it helps us estimate some degree of uncertainty in the future (something we don't know anything about).\n",
    "\n",
    "Another way for thinking about inferential statistics goes as follows. We have evidence about the characteristics of all of the observations in our sample, the true parameters of the population are unkown but we can approximate an answer to the true parameters if the sample is large enough. In this scenario, we would like to make an inference or educated guess about the truth. \n",
    "\n",
    "Three broad goals of statistical inference are parameter estimation, prediction, and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s29745.pcdn.co/wp-content/uploads/2019/05/Airbnb-Rentals-in-Melbourne.jpg.optimal.jpg\" alt=\"melbourne\" width=\"600\"/>  \n",
    "\n",
    "We will continue using the Melbourne Airbnb data we started using last class. This time though, we will skill the cleaning part build on the statistical concepts used in our previous lesson. Here is a refresher on the data and what we did to it.\n",
    "\n",
    "1. The data is scraped by a tool called [Inside Airbnb](http://insideairbnb.com/) and we are using a version which was last scraped in 2018.\n",
    "2. The data contains over 20k rows and each one of them represents a listing for a place to rent.\n",
    "3. We got rid of data above 400 and below 25.\n",
    "4. We dealt with missing values by using the median as a proxy.\n",
    "5. We got rid of more than half of the variables.\n",
    "\n",
    "We will load the saved version `mel_airbnb_ready.csv`, do a bit of additional cleaning and preparation, and use that data for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../lesson_5/mel_airbnb_ready.csv', parse_dates=['host_since'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick view of the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our columns are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with the extreme values in the cleaning fee and the minimum night by assigning new values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['price', 'minimum_nights', 'cleaning_fee']].sort_values(by=['minimum_nights', 'cleaning_fee', 'price'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['price', 'minimum_nights', 'cleaning_fee']].sort_values(by=['cleaning_fee', 'minimum_nights', 'price'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_night_mask = (df['minimum_nights'] > 499) #filter out the extreme\n",
    "df.loc[min_night_mask, 'minimum_nights'] = df['minimum_nights'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stays where cleaning costs more than double the stay per night should be dealt with too. We will create a mask and assign the median of the array to those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mask = ((df['minimum_nights'] < 4.00) & (df['price'] < df['cleaning_fee'])) \n",
    "df.loc[clean_mask, 'cleaning_fee'] = df['cleaning_fee'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min_price_stay'] = (df['price'] * df['minimum_nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new column to represent the true price of a stay at an Airbnb in Melbourne. This should be the `Price per Night * Minimum Nights per Stay + Cleaning Fee = True Price`. We will name our new column, `true_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_price'] = (df['price'] * df['minimum_nights'] + df['cleaning_fee'])\n",
    "df['true_price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaner_melb_abnb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Intro to SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scipy](https://www.fullstackpython.com/img/logos/scipy.png)  \n",
    "**Source:** [SciPy](https://www.scipy.org/scipylib/index.html)\n",
    "\n",
    "SciPy stands for Scientific Python and it is a library with the goal of making scientific data analysis easier. The name SciPy is often used as a reference to the main python ecosystem of libraries availables for data analysis, of which the most widely known libraries are pandas, numpy, matplotlib, and, of course, SciPy.\n",
    "\n",
    "SciPy has many tools and submodules in it with the most famous ones being\n",
    "\n",
    "- `linalg` --> a sub-module for linear algebra applications\n",
    "- `stats` --> the statistical sub-module we explored in the previous lesson\n",
    "- `optimize` --> great tool for optimization problems\n",
    "- `signal` --> for signal processing\n",
    "- many more...\n",
    "\n",
    "For our purposes, we will be using the `stats` sub-module for hypothesis testing and other applications.\n",
    "\n",
    "\n",
    "You should have SciPy already installed in your computer, but just in case you need to reinstall it again, here is the code to so:\n",
    "```sh\n",
    "\n",
    "!pip install scipy\n",
    "```\n",
    "\n",
    "Let's import the `stats` sub-module from the scipy library and then continue on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use this if the command below fail\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![florence](https://cdn8.openculture.com/wp-content/uploads/2016/03/27203643/1024px-Nightingale-mortality.jpg)  \n",
    "**Source:** Florence Nightingale\n",
    "\n",
    "Statistical distributions are a tool to represent our dataset and a way to approximate reality. Distributions allow us to see how the likelihood or probability or different measurements (e.g., prices, weights, temperature, ect.) are distributed. The mountain or peak of a distributions show the most likely data points we will encounter, while the tails of a distribution let us see the most unlikely events.\n",
    "\n",
    "Distributions are usually visualised using histograms, and histrograms represent information or data by grouping each observation into bins. Aside from allowing us to visually inspect the data, histograms also give us a way to estimate how likely we are to find an observation taken at random, inside any of the bins. The more observations we gather about X phenomena, e.g. the more price per night data for new listings in our Airbnb dataset, the more spread out and smooth our histogram might become.\n",
    "\n",
    "We are also able to pass curve through a histogram to show a smooth approximation of the data point to a given bin, and we can also represent the probability of seeing a data point that we don't have in our sample.\n",
    "\n",
    "The kind of distribution we have been referring to so far is that one where there is only one peak, and to an extent, only one shape. Because different distributions share different kinds of features, there are other types of distributions such as:\n",
    "\n",
    "### Discrete distributions\n",
    "\n",
    "- There are discrete distributions where all outcomes are equaly likely (e.g. picking a card from a deck or rolling a die), these are said to follow a **Uniform distribution**.\n",
    "- Other discrete distributions, such as those that only have two potential outcomes, true or false, yes or no, follow what we call a **Bernoulli distribution**.\n",
    "- When we calculate many yes or no distributions and plot them on a graph, we have what is called a **Binomial distribution**.\n",
    "- There are also distributions that allow us to calculate the probability of very rare events, such as the **Poisson distribution**.\n",
    "\n",
    "\n",
    "### Continuous distributions\n",
    "\n",
    "Continuous distributions can be represented more easily by a curve as opposed to binned bars, and that is because the data point in such an array usually has float values (e.g. price per house, income, termperature, height, weight, etc.).\n",
    "- The most common distribution we see of the continuous kind is the **normal distribution**. Which is very closely represented by natural events such as precipitaion, snow measurements in inches, etc. Even though the of a normal distribution resembles well rounded mountain, we can often encounter many rare events called outliers, and we can also be presented with very little information about a phenomenon (e.g., there was very little evidence of COVID-19 prior to 2019). To represent different events with distributions, we have a few options\n",
    "- **Student's T distribution** is a special distribution that allows us to accommodate extreme values very well. Think of this distribution as a mountain with a smaller peak and fatter flat sides. The lower number of elements would make the occurance of the extreme values represent a higher population than they would otherwise\n",
    "- **Chi-Squared distribution** always contain non-negative numbers and also always asymmetric. This means that they don't have a typical mountain and will always start after 0 in the origin of any x-y coordinates. This distribution is very useful for hypothesis testing but it often lacks in its inability to truly represent reality.\n",
    "- **Exponential distributions** represent rapidely changing events such as epidemics or stock market crashes.\n",
    "- **Logistic or Sigmoid distribution** allows us to forecast different events and also determine the cut off point where an event happens. This distribution is oftern used in classification algorithms to tell us whether an image has a cat or a dog, and email is spam or not, or if we have rain or no rain.\n",
    "\n",
    "Let's plot each one of these distributions to see what they look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "\n",
    "The normal distribution tells us that the distribution of our data has a peak where the mean, the median, and the mode are the same and the standard deviation, a.k.a. the dispersion of our data, is normal or smooth as well. We will first generate a random array using the `norm` sub-module from SciPy's stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('cleaner_melb_abnb.csv', parse_dates=['host_since'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rvs method generates random variations based on a mean and an std\n",
    "normal_dist = norm.rvs(loc=0, scale=1, size=100, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal_dist, bins=23, edgecolor='white', alpha=0.5)\n",
    "plt.title(\"This is a Normal Distribution\")\n",
    "plt.xlabel(\"Bins containing a range for the data\")\n",
    "plt.ylabel(\"The top of the Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the bins and add a line which shows us the probability of an event occuring at a point where no data has been observed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(normal_dist, bins=60, kde=True,\n",
    "                  color='skyblue', hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "\n",
    "plt.title(\"This is a Normal Distribution with Density Line\")\n",
    "plt.xlabel(\"Bins containing a range for the data\")\n",
    "plt.ylabel(\"The top of the Bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berni_dist = bernoulli.rvs(size=10000,p=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= sns.distplot(berni_dist, kde=False,\n",
    "                 color=\"skyblue\", hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "\n",
    "plt.xlabel('Bernoulli Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"The Yes or No Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform\n",
    "\n",
    "Uniform distributions are usually flat and squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_dist = uniform.rvs(loc=30, scale=4, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(uni_dist, bins=30,\n",
    "                  kde=True, color='red',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':0.5})\n",
    "\n",
    "plt.xlabel('Uniform Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Plotting a Flat Mountain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expo_dist = expon.rvs(scale=1,loc=0,size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(expo_dist, kde=False, bins=100, color='skyblue',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':0.5})\n",
    "\n",
    "plt.xlabel('Exponential Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Expo Func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.linspace(25, 30, 100)\n",
    "exp = np.exp(arr1)\n",
    "plt.plot(arr1, exp)\n",
    "plt.title(\"Very Expo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_dist = poisson.rvs(mu=3, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(poisson_dist, bins=30, kde=False,\n",
    "             color='green', hist_kws={\"linewidth\": 15,'alpha':0.4})\n",
    "\n",
    "plt.xlabel('Poisson Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Poisson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100) \n",
    "z = 1/(1 + np.exp(-x)) \n",
    "  \n",
    "plt.plot(x, z) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\") \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/null_hypothesis.png\" alt=\"null h\" width=\"300\"/>  \n",
    "**Source:** https://xkcd.com/892/\n",
    "\n",
    "As data analysts, we will often want to test a variety of questions with our data to improve and enhance the decision-making process of our organisations. We will also want to know whether these questions hold true against the evience we currently have, and to do this, we turn to the inferential side of statistics to compare a variety of methods suitable for different data types.\n",
    "\n",
    "Since we assume the data we have comes from some type of real distribution in the actual population, hence, when we answer a hypothesis test we are doing so in terms of the whole population and not the sample from which the data was taken. Because of the large space of questions we can asks for different data, we will have different flavors of hypotheses **tests** to choose from. Before we move on, let's define what a hypothesis is in a general way.\n",
    "\n",
    "> \"A hypothesis is a question, premise, claim or idea that we want to test using data\"\n",
    "\n",
    "The most basic elements of a hypothesis are\n",
    "- $H_{0}$ --> (pronounced H of not) is the Null Hypothesis or **the idea that we want to disprove.** You can also think of this as the status quo or what has already been accepted by the majority but not proven false yet.\n",
    "- $H_{A}$ --> (pronounced H of A) is the alternative hypothesis we want to test and hopefully prove right. It is also called the research hypothesis.\n",
    "- Statistical significance --> the measure or place where we draw the line to be able to say that our results are strong enough to disprove what is currently accepted.\n",
    "- Reject $H_{0}$ --> this is the result we get when we do find evidence against the status quo or accepted view or idea.\n",
    "- Fail to reject $H_{0}$ --> this is the result we get when we are unable to disproved the accepted idea.\n",
    "- Level of Confidence --> usually set to 95%. Is our degree of confidence in our decision. For example, if we are rejecting a null hypothesis we could say, \"I am 95% confident that rejecting the null hypothesis is a valid conclusion.\n",
    "- $\\alpha$ --> Is the level of significance of our decision. This is calculated by subtracting 95% from 1 (e.g. $\\alpha = 1 - 95%$ and it is the explicit threshold by which the probability we get from our result needs to be for us to make our decision. For example, the probability estimate generated from our hypothesis test has to be below 0.05 for it to be meaningful to us.\n",
    "- $p-value$ --> Is the probability or result we get from a hypothesis test. It allows to prove or dispruve our assumptions. The way we use p-values is by comparing this result with our level of significance $\\alpha$. In other words\n",
    "    - If $p-value > \\alpha$ we **fail to reject** the Null Hypothesis\n",
    "    - If $p-value < \\alpha$ we **reject** the Null Hypothesis. Another way of wording this: There is a 5% chance that two identical distributions would have produce the results we are observing.  \n",
    "Formal definition of the p-value\n",
    "> Probability of obtaining a sample more extreme than the ones observed in your data, assuming that the Null Hypothesis is true.\n",
    "\n",
    "Think about the last two elements above in terms of the guinness world records, until someone comes and beats a record in that book, we cannot disprove him/her/they/them as the true record holder. The case is the same for hypothesis testing.\n",
    "\n",
    "Notice from the above statements that we can infer that no hypothesis is ever fully true just not proven false until something better comes up. Always keep this in ming as you move along your data analytics journey. Also, since in order test a hypothesis we will need past data, questions that lack data will have no foundation to be tested on. For example\n",
    "\n",
    "- Do eagles fly faster than armadillos? Until armadillos can fly and we gather data about how they do, this will remain an untestable hypothesis.\n",
    "- Will robots take over the world? No data exists on this. There are a lot of fun science fiction movies though.\n",
    "- Will aliens invade us 10 years? ...\n",
    "\n",
    "The steps for hypothesis testing roughly follow the structure below.\n",
    "1. What kind of data do we have: Categorical or Quantitative?\n",
    "2. How many samples am I comparing?\n",
    "3. We then need to formulate our question. \n",
    "    - Is the average price of all Airbnb listings in Melbourne truly 124.7?\n",
    "        - $H_{0}:\\mu_{0} = 124.7$ \n",
    "        - $H_{A}:\\mu_{0} \\neq 124.7$\n",
    "        - The two hypotheses above can be thought of as mathematical opposite questions.\n",
    "4. Which test do we need to use in order to test our hypothesis?\n",
    "5. Test your hypothesis with your chosen test.\n",
    "6. Use that information to make your decisions.\n",
    "\n",
    "### Sides of a hypothesis\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/862/1*VXxdieFiYCgR6v7nUaq01g.jpeg\" alt=\"h-sides\" width=\"700\"/> \n",
    "\n",
    "\n",
    "### Testing options given your data\n",
    "\n",
    "| Comparison | Data you have | Test Available |\n",
    "|-----|---------|----|\n",
    "| 1 Sample vs Known Population | categorical | Binomial Test |\n",
    "| 1 Sample vs Known Population | numerical | 1 Sample t-test |\n",
    "| 2 Samples | categorical | Chi-Square |\n",
    "| 2 Samples | numerical | 2 Sample t-test |\n",
    "| More than 2 | categorical | Chi-Square |\n",
    "| More than 2 | numerical | ANOVA and/or Tukey |\n",
    "\n",
    "\n",
    "Another way of reframing hypothesis testing is by asking\n",
    "\n",
    "> What is the probability that the difference I am observing is due to chance?\n",
    "\n",
    "Lastly, remember that the goal of hypothesis testing is to disprove the Null Hypothesis.\n",
    "\n",
    "We will now state a variety of hypotheses, test them using their appropriate method, and explain that method in detail as we go along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first question - What kind of \n",
    "\n",
    "## Example 1\n",
    "\n",
    "Imagine we know that the average price per stay for the last five years in all of Melbourne's Airbnb listings was 124/night. Then, the next year we get new data and the average price goes to 123, and the next it goes to 122,, then 121, and on it goed. Given this information, we would start loosing confidence that true value, say 124.7, is not the true average but a value due to chance. To test this, we would use a 1 Sample t-test.\n",
    "\n",
    "> The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesized population mean.\n",
    "\n",
    "Here is how we would go about testing our hypothesis in python with the data we actually have.\n",
    "\n",
    "It is good practice to always state your questions before proceeding to code the test, and also to visualise the distribution of the variable.\n",
    "\n",
    "- $H_{0}:\\mu_{0} = 124.7$\n",
    "- $H_{A}:\\mu_{0} \\neq 124.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].plot(kind='hist', bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first argument is our array\n",
    "# the second is our hypothesised population mean\n",
    "\n",
    "tstat, pval = ttest_1samp(df['price'], 123)\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('We Fail to Reject the Null Hypothesis!')\n",
    "else:\n",
    "    print('We Reject the Null Hypothesis!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1 sample t-test allows us to test whether the distribution of a variable is significantly different than our chosen mean. In our case, a value of 123 gives us enough confidence to say that the average mean is not a conclusive figure and thus, we can reject the Null Hypothesis.\n",
    "\n",
    "**Note:** If we don't select a confidence level, the default will be `0.05`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Here we want to compare two sample distributions in our dataset, which we assume are normally distributed. We take two groups given a condition and we compare the mean of both arrays.\n",
    "\n",
    "Definition:\n",
    "\n",
    "> The purpose of the test is to determine whether there is statistical evidence that the mean difference between paired observations on a particular outcome is significantly different from zero.\n",
    "\n",
    "Our confidence level is again `0.05`. This means that any p-value below that would allows to see whether there is a significant difference between the average price of these two categories or not.\n",
    "\n",
    "- $H_{0}:$ samples average are the same\n",
    "- $H_{A}:$ samples average is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate the arrays and assign them to a variable\n",
    "super_host = df.loc[df['host_is_superhost'] == 't', 'price']\n",
    "regular_host = df.loc[df['host_is_superhost'] == 'f', 'price']\n",
    "\n",
    "super_host.plot(kind='hist', bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_host.plot(kind='hist', bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat, pval = ttest_ind(super_host, regular_host, equal_var=False)\n",
    "\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('We Fail to Reject the Null Hypothesis!')\n",
    "else:\n",
    "    print('We Reject the Null Hypothesis!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there is a significant difference between the average of both variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "Analysis of Variance (ANOVA) allows us to compare the average value of more than 2 samples to see whether there is a statistically significant difference between them.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "- Samples are independent and identically distributed.\n",
    "- Each of the multiple samples are normally distributed.\n",
    "- Observations in each sample have the same variance.\n",
    "\n",
    "  \n",
    "\n",
    "- $H_{0}:$ samples averages are the same\n",
    "- $H_{A}:$ samples averages are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['room_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_home = df.loc[df['room_type'] == 'Entire home/apt', 'min_price_stay']\n",
    "private_room = df.loc[df['room_type'] == 'Private room', 'min_price_stay']\n",
    "shared_room = df.loc[df['room_type'] == 'Shared room', 'min_price_stay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first compare the prices charged by the three room types in our dataset. Notice how the output of the test is a tuple with the test statistic, a values we can compare against a variety of probabilities in a table, and the p-value. What we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pval = f_oneway(entire_home, private_room, shared_room)\n",
    "\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('We Fail to Reject the Null Hypothesis!')\n",
    "else:\n",
    "    print('We Reject the Null Hypothesis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df['price']\n",
    "min_price_stay = df['min_price_stay']\n",
    "full_pay = df['true_price']\n",
    "clean_fee = df['cleaning_fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pval = f_oneway(price, min_price_stay, clean_fee)\n",
    "\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('We Fail to Reject the Null Hypothesis!')\n",
    "else:\n",
    "    print('We Reject the Null Hypothesis!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The caveat with ANOVA's is that they don't tell us which sample is more signficantly different than other. To accomplish this level of granularity, we need another test called the Tukey test.\n",
    "\n",
    "The Tukey's range test compares several numerical samples (more than 2) without increasing the probability of a false positive (e.g. a false interpretation that was taken as true).\n",
    "\n",
    "To compute it, we need to append all three arrays into a vertical one, along with the names of each array. Let's first import the `pairwise_tukeyhsd` model from statsmodel and then calculate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_stack = np.concatenate([price, min_price_stay, clean_fee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['price'] * len(price) + ['min_price_stay'] * len(min_price_stay) + ['clean_fee'] * len(clean_fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_test = pairwise_tukeyhsd(vertical_stack, labels, 0.05)\n",
    "print(tukey_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4\n",
    "\n",
    "Pearson Correlation\n",
    "\n",
    "We saw the pearson correlation function in the last class, but what does it really test? The goal of this test is see whether there is a linear relationship between your numerical variables or not. That means that we want to know whether the variables have dependency or not with one another. \n",
    "\n",
    "\n",
    "- $H_{0}:$ samples average are independent\n",
    "- $H_{A}:$ there is dependency between the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pval = pearsonr(price, min_price_stay)\n",
    "\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6\n",
    "\n",
    "Spearmanâ€™s Rank Correlation\n",
    "\n",
    "Is used to discover the strength of a link between two sets of data. For example, the price per night versus the cleaning fee of a listing.\n",
    "\n",
    "- $H_{0}:$ samples average are independent\n",
    "- $H_{A}:$ there is dependency between the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "stat, pval = spearmanr(price, clean_fee)\n",
    "\n",
    "print(\"Our P-values is %.4f\" % pval)\n",
    "\n",
    "if pval > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inferential statistics help us deal with uncertainty and estimate the parameters of the true population\n",
    "- Distributions are a key component to understand how we model reality\n",
    "- A hypothesis test helps us disprove statements and knowledge that has not been disproved yet\n",
    "- SciPy's stats model has plenty of useful functionalities for statistical analysis\n",
    "- There many, many hypothesis tests and we have only scratched the surface here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herne, H., & Huff, D. (1973). _How to Lie with Statistics_. Applied Statistics, 22(3), 401. doi: 10.2307/2346789\n",
    "\n",
    "Downey, Allen B. _Think Stats: Exploratory Data Analysis in Python_. Green Tea Press, 2014.\n",
    "\n",
    "Lock, Robin H., et al. _Statistics: Unlocking the Power of Data_. John Wiley & Sons, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would really appreciate it if you could please provide us with your feedback from this session by filling a couple of question.\n",
    "\n",
    "> ## [Survey](https://docs.google.com/forms/d/e/1FAIpQLScsvD4KHRJZPstPHZlAv5ibNOvL9ioSgQRCrIIqwQzW_mG8Og/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
