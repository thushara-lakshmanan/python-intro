{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataviz](https://www.boostlabs.com/wp-content/uploads/2019/09/10-types-of-data-visualization.jpg)\n",
    "\n",
    "**Source:** https://boostlabs.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Recap of lesson 3  \n",
    "II. Learning Outcomes  \n",
    "\n",
    "\n",
    "__Outline for the session:__\n",
    "\n",
    "1. Introduction to Data Visualisation\n",
    "2. Quantitative and Qualitative DataViz\n",
    "3. Types, Do's and Dont's\n",
    "    - Static DataViz\n",
    "    - Interactive DataViz\n",
    "4. Data Cleaning and Preparation\n",
    "    - Load\n",
    "    - Inspect\n",
    "    - Clean & Prepare\n",
    "5. Introduction to Matplotlib\n",
    "6. Introduction to Seaborn\n",
    "7. Introduction to Bokeh\n",
    "8. Data Visualisation Best Practices\n",
    "9. Summary\n",
    "10. References\n",
    "11. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Recap of lesson 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the las lesson we have covered pandas in great lenght, and still, we have not even began to scratch the surface of what this powerful tool can do. Some of keypoints to take away from last lesson, and some of which will be reinforced again in this one, are:\n",
    "\n",
    "- pandas provides two fantastic data structures for data analysis, the DataFrame and the Series. Series are one-dimensional arrays that are similar to NumPy but with more functionalities. DataFrames are 2-dimensional data structures that allows to manipulate data with ease.\n",
    "- We can slice and dice these data structures to our hearts content but we have to keep in mind the inconsistencies that we might find in different datasets.\n",
    "- We should always begin by inspecting our dataset immediately after loading it into memory. pandas provides methods such as info, describe, and isna that work very well and allow us to look at the underlyiing information about the dataset.\n",
    "- Don't try to learn all the tools inside pandas but rather explore the ones you need as the need arises. You can also explore them slowly and build and practice with different datasets to build an intuition behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the lesson you will have learned:\n",
    "\n",
    "1. What is data visualisation and how does it fit in the data analytics cycle.\n",
    "2. How to differentiate between quantitative vs qualitative data for visualisation purposes.\n",
    "3. When to use static vs interactive data visualisations.\n",
    "4. How to get started creating your own visualisations using matplotlib, seaborn and bokeh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualisation, more than being part art and part science, is one of the key components of the data analytics cycle. People have different learning styles and to be able to convey information in a more accessible way, sometimes it is better to do so through visualisations rather than tables and written text.\n",
    "\n",
    "To quote [Tableau](https://www.tableau.com/), a company making data visualisation more accessible to everyone, \"Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.\"\n",
    "\n",
    "Data Visualization as a field of study has been on the rise for over many decades now --if not centuries-- and it is an exciting area to be a part of. Organisations such as the [Data Visualization Society](https://www.datavisualizationsociety.com/), FreeCodeCamp, and others, have extensive information on how to go beyond simple data visualisation, should that be something that interests your. If you would like to read more about data visualisation and what you can do with it, check out this [medium site](https://medium.com/nightingale).\n",
    "\n",
    "Python has a wide variety of visualisation tools available for static and interactive, quantitative and qualitative, time series and geographic data visualisation, and some of the most-widely used libraries for these purposes, to date, are the following ones:\n",
    "\n",
    "- [matplotlib](https://matplotlib.org/) --> highly customisable and long-term contender in the dataviz arena\n",
    "- [seaborn]() --> beautiful data visualisation library that is easy to use and fast\n",
    "- [bokeh]() --> great (and beautiful) tool for interactive data visualisation\n",
    "- [plotly]() --> bokeh's top contender\n",
    "- [altair]() --> beautiful data visualisation library based on the grammar of graphics philosophy\n",
    "- [plotnine]() --> data visualisation library based on R's ggplot2\n",
    "\n",
    "While we can't possibly cover all them in this lesson, you will find that the concepts you will learn thoughout this section will help you transition your knowledge from one tool to the next in a seamless fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantitative and Qualitative DataViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to the data visualisation stage of the data analytics cycle, we should always keep in mind the nature of the data we would like to visualise. If we want to see relationships (correlations between variables) we might only choose quantitative variables for our visualisations. If we want to show a specific theme in our dataset, e.g. gender differences, customer type, or potential customer, we might just opt for visualising frequencies in qualitative data. In contrast, if we want to show the relationship of variables given a specific group in our dataset (e.g. income differences by gender), we would choose a combination of qualitative and quantitative variables.  \n",
    "\n",
    "Speaking of choosing a combination of variables, this is often another good questions analysts tend to spend time on. Should we want to show one particular variable in isolation or a group of variables would depend on the context of the question at hand, and in which part of the data analytics cycle you as an analyst are on. For example, boxplots can be used extensively during the cleaning stage to detect outliers, and they can also be used during the presentation stage to showcase the distribution of some of your most important variables.\n",
    "\n",
    "Now that we are aware of these subtle differences, how do we know which visualisation to pick? The answer is that it will depend on the context of your task, and on how much information you would like to convey in your visualisation. As you decompose a task to choose the best course of action for your visualisation, keep the following diagram in mind from ActiveWizards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![choosing](https://activewizards.com/content/blog/How_to_Choose_the_Right_Chart_Type_[Infographic]/chart-types-infographics04.png)  \n",
    "**Source:** https://activewizards.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Types, Do's and Dont's\n",
    "\n",
    "![image](http://truth-and-beauty.net/content/1-projects/30-the-rhythm-of-food/01-apricot-highlight.png)  \n",
    "**Source:** http://rhythm-of-food.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other factors we need to keep in mind when we visualise data. The most important ones are\n",
    "\n",
    "1. what are we trying to visualise\n",
    "2. what kind of data do we have? Is it quantitative, qualitative, both?\n",
    "3. who will be seeing or evaluating our visualisations?\n",
    "4. will our audience benefit from interacting with the visualisation, or will a static representations suffice?\n",
    "\n",
    "Why should we pay attention to the data type? Visualisations with quantitative data will help us show relationships in our data, that is, what happens to the movement of one variable when compared to another. In the case of categorical data, we might want to show the frequency with which an event has happened in the past, for example, how many times did a customer go to the ice-cream shop last week in comparison to the previous one. This kind of information can be represented can be represented as a bar chart since it contains discrete numbers.\n",
    "\n",
    "| When? | How many? | Who? |\n",
    "|-----|---------|----|\n",
    "|week1 | 2 | customer 1 |\n",
    "|week2 | 1 | customer 1 |\n",
    "|week1 | 2 | customer 2 |\n",
    "|week3 | 3 | customer 1 |\n",
    "|week2 | 2 | customer 2 |\n",
    "|week3 | 1 | customer 2 |\n",
    "\n",
    "\n",
    "What kind of data do you have? The kind of data you have -- whether quantitative, qualitative, time series or geographical -- will dictate the range of visualisations you can do with it. At the same time, the more data types you have, the broader the range of visualisations you can create. Geographical data, in particular, is perfect for using it in combination with other data types. See, for example, the map by Mike Bostock below, which shows the population of the United States by county.\n",
    "\n",
    "![map](pictures/map.png)  \n",
    "**Source:** https://bost.ocks.org/mike/bubble-map/\n",
    "\n",
    "Here we have geographical data and discrete quantitative data. The larger the population of a county, in whole numbers of course, the larger the size of the bubble.\n",
    "\n",
    "Who will be seeing or evaluating your visualisation? This point is crucial, if the visualisation is just for us to understand a specific part of our dataset, then we might not have to create it with as much detail as we would for an audience we were presenting information to. After all, we know this visualisation will only be seen by us. In contrast, if we create a data visualisation to help our boss evaluate a specific analysis, or for an audience of colleagues working on a particular project, we want to make it as easy as possible to understand the message we are trying to convey, and in the appropriate context.\n",
    "\n",
    "Context is essential for clarity. If we are presenting a technical insight to a non-technical audience, the message and the presentation will need to be adjusted accordingly. The reach of our message sometimes is more important than the technical little details of it.\n",
    "\n",
    "Another important aspect to keep in mind is interactivity. As analysts, we should always ask ourselves, will our message reach our audience better if they were able to interact with the visualisation? The reason behind this can be captured in a very famous quote by Benjamin Franklin.\n",
    "\n",
    "> \"Tell me and I forget. Teach me and I remember. Involve me and I learn.\" ~ Benjamin Franklin\n",
    "\n",
    "If the goal of our visualisations is to teach something to our audience, chances are that allowing them to interact with our visualisation will do just that. Let's talk a bit more about static and interactive visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Static DataViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static data visualisations are those meant to show one or several specific facts about the data. They help us convey a message and often used closely with other narratives. For example, the New York Times is one of the most famous new agencies in the world not just for the top content they manage to craete and provide to the masses, but also for the beautiful visualisations one can find in their mountains of information.\n",
    "\n",
    "Static visualisations are also often embedded into inforgraphics to carry a message further. Think about graphs displayed inside broshures that tell us to buy a fragance or a particular type of cutlery, some often say \"75% of those who purchased these products have experienced...\". Watch out for those :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Interactive DataViz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive data visualisations tell different stories while letting the users pick which one they would like to view or understand better. These kinds of visualisations can be very powerful tools not only to convey messages to the masses but also to provide top-notch educational content for others.\n",
    "\n",
    "Involving your audience through interactive visualisations can be a much more involved process. A static visualisation can be saved and shared with many in a matter of minutes. Interactive visualisations, on the other hand, might require a web application to work. One of the tools we will explore in this lesson, bokeh, allows us to move this applications to the browser almost seemlessly. Dashboards and other tools would or can require a bit more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Do's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating data visualisations, it is important to keep in mind the following **Do's**.\n",
    "\n",
    "1. Label your axes where appropriate\n",
    "2. Add a title\n",
    "3. Use color appropriately. Showcase what you need, not every data point\n",
    "4. Use full axis and maintain consistency with different graphs shown in parallel\n",
    "5. Ask others for their opinion\n",
    "6. Pass the squint test (blurry viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Dont's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as there are many **Do's** in data visualisations, there are also many **DONT's**. Let go over a few of them together.\n",
    "\n",
    "1. Don't use too much color  \n",
    "<img src=\"https://serialmentor.com/dataviz/pitfalls_of_color_use_files/figure-html/popgrowth-US-rainbow-1.png\" alt=\"bad pie\" width=\"400\"/>   \n",
    "\n",
    "2. Don't use unmatching percentages  \n",
    "<img src=\"http://livingqlikview.com/wp-content/uploads/2017/04/Worst-Data-Visualizations-02.jpg\" alt=\"bad percentages\" width=\"400\"/>  \n",
    "\n",
    "3. Don't try to put everything in one graph  \n",
    "<img src=\"http://livingqlikview.com/wp-content/uploads/2017/04/Worst-Data-Visualizations-07.jpg\" alt=\"bad pie\" width=\"400\"/>  \n",
    "\n",
    "4. Trend lines need time not categories  \n",
    "<img src=\"http://livingqlikview.com/wp-content/uploads/2017/04/Worst-Data-Visualizations-03.jpg\" alt=\"bad lines\" width=\"400\"/>  \n",
    "\n",
    "5. Don't make your chart data unreadible  \n",
    "<img src=\"http://livingqlikview.com/wp-content/uploads/2017/04/Worst-Data-Visualizations-04.jpg\" alt=\"bad text\" width=\"400\"/>  \n",
    "\n",
    "6. Don't make no sense  \n",
    "<img src=\"https://i.insider.com/51cb1c3e69bedd713300000e?width=1200\" alt=\"bad sense\" width=\"400\"/>  \n",
    "\n",
    "7. Don't deceive your audience with different intervals and axes  \n",
    "<img src=\"https://i.insider.com/51cb25fa69beddcd4f000005?width=1200\" alt=\"bad intervals\" width=\"400\"/>  \n",
    "\n",
    "8. Axes betrayal  \n",
    "<img src=\"https://i.insider.com/51cb2721eab8ea1d33000004?width=1200\" alt=\"bad percentages\" width=\"400\"/>  \n",
    "\n",
    "\n",
    "**Source 1:** taken from Fundamentals of Data Visualization by Claus O. Wilke. Data source is US Census Bureau  \n",
    "**Source 2:** Figures 2, 3, 4, and 5 were taken from [QlikView](http://livingqlikview.com/the-9-worst-data-visualizations-ever-created/)  \n",
    "**Source 3:** Figures 6, 7, and 8 were taken from [Business Insider](https://www.businessinsider.com.au/the-27-worst-charts-of-all-time-2013-6?r=US&IR=T#did-anyone-learn-anything-by-looking-at-this-pseudo-pie-chart-what-do-these-colors-even-mean-why-is-it-divided-into-quadrants-well-never-know-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we will be working with a dataset containing weather data for Australia from 2007 to 2017. The nice thing about this dataset is that, although it has been pre-processed and it is quite clean, there is still a fair amount work to do regarding missing values, outliers and others. Once you are out in the real world, you will encounter a plethora of datasets with different data types per column, incomprehensible data structures, and, not to scare you, many other issues such as different formats within different elements inside different structures. In other words, data that might look like this:\n",
    "\n",
    "![spaghetti](https://media.giphy.com/media/dZRlFW1sbFEpG/giphy.gif)  \n",
    "**Source:** https://foodbinge.tumblr.com/post/26122779310\n",
    "\n",
    "**About the data:**  \n",
    "This dataset contains weather information from many of the weather stations around Australia. For most weather stations, we have about 365 observations for the years 2007 to 2017. More information about the dataset can be found in the [Australian Bureau of Meteorology website](http://www.bom.gov.au/climate/dwo/), and below you can find a short description of the variables in the dataset.\n",
    "\n",
    "**Variables info:**\n",
    "- Date --> day, month, and year of the observation, each weather station has its own\n",
    "- Location --> location of the weather station\n",
    "- MinTemp --> minimum temperature for that day\n",
    "- MaxTemp --> maximum temperature for that day\n",
    "- Rainfall --> the amount of rainfall recorded for the day in mm\n",
    "- Evaporation --> the so-called Class A pan evaporation (mm) in the 24 hours to 9am\n",
    "- Sunshine --> the number of hours of bright sunshine in the day\n",
    "- WindGustDir --> the direction of the strongest wind gust in the 24 hours to midnight\n",
    "- WindGustSpeed --> the speed (km/h) of the strongest wind gust in the 24 hours to midnight\n",
    "- WindDir9am --> direction of the wind at 9am\n",
    "- WindDir3pm --> direction of the wind at 3pm\n",
    "- WindSpeed9am --> wind speed (km/hr) averaged over 10 minutes prior to 9am\n",
    "- WindSpeed3pm --> wind speed (km/hr) averaged over 10 minutes prior to 3pm\n",
    "- Humidity9am --> humidity (percent) at 9am\n",
    "- Humidity3pm --> humidity (percent) at 3pm\n",
    "- Pressure9am --> atmospheric pressure (hpa) reduced to mean sea level at 9am\n",
    "- Pressure3pm --> atmospheric pressure (hpa) reduced to mean sea level at 3pm\n",
    "- Cloud9am --> fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many\n",
    "- Cloud3pm --> fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n",
    "- Temp9am --> temperature (degrees C) at 9am\n",
    "- Temp3pm --> temperature (degrees C) at 3pm\n",
    "- RainToday --> boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n",
    "- RISK_MM --> the amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\".\n",
    "- RainTomorrow --> did it rain the following day?\n",
    "\n",
    "The dataset and the information for the variables was taken from Kaggle, and you can find out more about the dataset either using the link above or the one below, and about Kaggle using the link below.\n",
    "\n",
    "Link --> https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "Now, let's get to loading, inspecting, and preparing our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be loading the dataset using the `pd.read_csv()` method we learned about during the last lesson, but before we load the data, we will see if we can figure inspect the first few rows of it with a helpful command line script called `head`. You might be wondering if this method resembles the `df.head()` method we learned in the last lesson, and the answer is yes. By passing a second parameter `head -n`, then a number `head -n 5`, and then the path to the file, we can print the amount of rows we specified to the console. This same command will run smoothly in Git Bash. For the Windows CMD you can use `type datasets\\files\\weatherAUS.csv -Head 5`.\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for windows users\n",
    "!type datasets\\files\\weatherAUS.csv -Head 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac users or windows users with Git Bash\n",
    "!head -n 5 ../datasets/files/weatherAUS.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we described the variables above, we can see that we have a date variable that we can parse as date type while reading the data into memory to save us some time. Let's go ahead and read in the data after we import our packages. We will assign our data to the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should you need a quick refresher on pd.read_csv(), just run the following code\n",
    "pd.read_csv??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a windows user, please don't forget to to use back slashes `\\` as opposed to forward ones `/` when raeding or saving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/files/weatherAUS.csv\", parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same methods we introduced in the last lesson to inspect our data. Here are the definitions again.\n",
    "\n",
    "- `df.head()` --> shows the first 5 rows of a DataFrame or Series\n",
    "- `df.tail()` --> shows the last 5 rows of a DataFrame or Series\n",
    "- `df.info()` --> provides information about the DataFrame or Series\n",
    "- `df.describe()` --> provides descriptive statistics of the numerical variables in a DataFrame\n",
    "- `df.isna()` --> returns True for every element that is NaN and False for every element that isn't\n",
    "- `df.notna()` --> does the opposite of `.isna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Australia collects, or the dataset contains, information from 49 weather stations around the country. Let's see how many missing values do we have in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we would like to see the percentage of missing values per row, we could use\n",
    "(df.isna().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T # remember that describe excludes all missing data by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the years we have data for, and how many values do we have per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.loc[df['Location'] == 'Sydney', 'Date'].dt.year.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Clean & Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and prepraring our data for analysis is the most crucial step of the data analytics cycle, and a non-perfect one as well. You will often find yourself coming up with different ways of reshaping and structuring the data, and thus, coming back to the **Clean & Prepare** stage of the cycle. This is completely normal and somewhat rewarding, especially since a lot of the times, insights come out when you least expext them.\n",
    "\n",
    "Let's begin by normalising our columns so that they have no spaces and are all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalise the columns\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.1 Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section we realised that we have quite a few missing values in some of the columns, and we should deal with them carefully. pandas provides a couple of great tools for dealing with missing values, and here are some of the most important ones.\n",
    "\n",
    "- `.dropna()` --> drops all or some missing values by column or row. Default is row\n",
    "- `.isna()` --> returns a boolean Series or DataFrame with a True for NaN values\n",
    "- `.notna()` --> does the opposite of `.isna()`\n",
    "- `.isnull()` --> same as `.isna()`\n",
    "- `.notnull()` --> same as `.notna()`\n",
    "- `.fillna()` --> allows you to fill missing values given a criterion\n",
    "\n",
    "When we encounter NaN values, our default action should never be to drop them immediate. We should first figure out why they might be missing through a thorough inspection of the data, and by looking at the documentation of how the data was gathered/acquired, should one exist and have enough detail of the data collection process, of course.\n",
    "\n",
    "One of the reasons we don't want to get rid of missing data immediately is that we might not be able to tell upon first inspection whether the missing values are due to an error with data collection or simply an instance that doesn't exist. For example, imagine you own a retail store that sells clothes for all kinds of weather and that you have a general survey that you send out to all of your customers. If you were to ask a customer in Latin America about whether they like to wear fluffy coats or regular coats whenever is winter season, they will probably leave that section blank because they don't experience a change of weather significant enough to buy that type of clothing. Hence, the missing value is not due to an error but rather an accurate observation.\n",
    "\n",
    "We do, however, might want to get rid of rows with too many missing values for each column. And this is, in fact, what we will do first by selecting to keep rows with at least 80% of the data in them. We can accomplish this with the `.dropna()` method of pandas and by passing in the parameter `thresh=18`, which tells pandas that if the row does not have at least 18 non-missing values, we don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will assign the new dataframe to a new variable\n",
    "\n",
    "dfmiss = df.dropna(thresh=18).copy()\n",
    "\n",
    "# and then check if there was a significant change\n",
    "(dfmiss.isna().sum() / dfmiss.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will tell us how many rows were deleted by the previous action. Remember that shape gives us back a tuple with `(rows_length, col_lenght)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] - dfmiss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmiss.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to pick a value that makes sense to fill in the missing values. For example, we might want to use the mean or the median as a general way and this won't distort results that much. What we need to keep in mind though is that, if there are too many outliers, the mean would give us an unrealistic representation of the missing values. We will deal with missing values in two ways, for the numerical values we will use the median, which is robust against outliers, and for the categorical variables we will use `ffill`. This last method allows us to forward fill a missing element with the preceding one.\n",
    "\n",
    "We will use a loop to do this.\n",
    "1. we will iterate over the columns\n",
    "2. use the column name to iterate over the dataframe\n",
    "3. check for two conditions, is it a float (as seen above) and does it have missing values\n",
    "4. if it is a float, we will fill in missing values with the median of that column\n",
    "5. else\n",
    "6. if the column contains data of type object and it has any missing values\n",
    "7. we will convert the column to a category type\n",
    "8. and forward fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more information on how fillna works, run this cell\n",
    "df.fillna??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in dfmiss.columns:\n",
    "    if (dfmiss[col].dtype == 'float64') & (dfmiss[col].isna().any()):\n",
    "        dfmiss[col].fillna(value=dfmiss[col].median(), axis=0, inplace=True)\n",
    "    elif (dfmiss[col].dtype == 'object') & (dfmiss[col].isna().any()):\n",
    "        dfmiss[col].astype('category', copy=True)\n",
    "        dfmiss[col].fillna(method='ffill', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are any remaining missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmiss.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since weather is time series data (e.g. data gathered over time), we will create additional date variables for visualisation purposes. Remember that when we have a date column of data type `datetime`, we can access all of the attributes available in our date column. We will use the same commands from our previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmiss['month'] = dfmiss['date'].dt.month\n",
    "dfmiss['year'] = dfmiss['date'].dt.year\n",
    "dfmiss['week'] = dfmiss['date'].dt.week\n",
    "dfmiss['weekday'] = dfmiss['date'].dt.weekday\n",
    "dfmiss['quarter'] = dfmiss['date'].dt.quarter\n",
    "dfmiss['day_of_week'] = dfmiss['date'].dt.day_name()\n",
    "dfmiss['week_or_end'] = dfmiss['weekday'].apply(lambda x: 'weekend' if x >= 5 else 'week_day')\n",
    "dfmiss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to represent the quarter variable as an object later on, so we will create a dictionary with the values we would like to change, and pass it to our Python's `.map()` method. A very useful fuction to map a function or set of values to a column or other data structure. We will assign the result to a new column called `qrt_cate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more info on how map works, please run this cell\n",
    "map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {1:'first_Q',\n",
    "           2:'second_Q',\n",
    "           3:'third_Q',\n",
    "           4:'fourth_Q'}\n",
    "\n",
    "\n",
    "dfmiss['qtr_cate'] = dfmiss['quarter'].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.1 Save your dataset for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we want to do is to reset the index of our dataframe and save its clean version for later use.\n",
    "\n",
    "We can use pandas method `.reset_index()` to reset the index. Notice the `drop=True`, if we do not make this parameter equal to True, pandas will assign the old index to a new column.\n",
    "\n",
    "The next method we will use is `.to_csv()`. By applying this method to a dataframe, all we need to do is to give the data a name (in quotation marks), and pass in the `index=False` parameter if we don't want the index to be added as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = dfmiss.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.to_csv('weather_ready.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Introduction to matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![matplotlib](https://matplotlib.org/_static/logo2_compressed.svg)  \n",
    "**Source:** https://matplotlib.org/\n",
    "\n",
    "matplotlib was first released in 2003 and since then, it has become one of the most widely used data visualisation tools in data analytics, data science, and scientific computing. One of the main reasons for its popularity is the extensive customisation capabilities of the plots one can build with it. Almost any chart you can think of, can be built with matplotlib. From 2D to 3D, frmo static to interactive, from statistical to geographical, and the list goes on.\n",
    "\n",
    "In this section, we will focus on how to get started creating data visualisations with matplotlib. We will cover the building blocks of this versatile tool, give examples of several use cases, and explore some data.\n",
    "\n",
    "One thing to keep in mind is that, we will be focusing on the `pyplot` module of matplotlib, so when we import matplotlib into our session, we will do so with its industry alias `plt`. For example, here are two ways for importing matplotlib's pyplot module to our session.\n",
    "\n",
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "```\n",
    "\n",
    "Another common functionality we would like to take advantage of is, that of being able to see our plots inline (i.e. inside our notebook as images) once we plot them. To do this, we will be using `%matplotlib inline`. This statement is called a magic command, and these magic commands are calls within IPython that provide different functionaltities to either a particular cell, line of code, or the entire notebook. In the case of `%matplotlib inline`, a cell containing this call will only need to be run once and then we can move on to making plots to our heart's content, knowing they will appear normally in our notebook.\n",
    "\n",
    "There are two common ways for generating plots using matplotlib, an object oriented way and a library.method-calling way. The latter resembles everything we have been doing so far with pandas and numpy (e.g. pd.something() and np.something(), respectively), while the former requires that we create an object to build our visualisations with. We will introduce both ways in this tutorial, but will favor the object oriented approach beyond this lesson, as it allows for a higher degree of customisation.\n",
    "\n",
    "Let's start by importing matplotlib, and inialising our magic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib has many plotting styles that we can take advantage of to make our plots ever better-looking. You can check the styles with the command below, but more information can be found on [matplotlib's styles website](https://matplotlib.org/3.1.0/gallery/style_sheets/style_sheets_reference.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xkcd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_ready.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach - Regular in general but also called MATLAB Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach we will see is the regular approach, also called MATLAB approach. It requires that we construct every graph using the `plt` module of matplotlib that we just imported.\n",
    "\n",
    "Our first task will be to examine the distribution of the variable, `maxtemp`. A histogram tells us the distribution of the data points inside a variable. Think of this a taking a list of values, grouping similar one into bins as tall as as the general frequencies of those numbers, and plotting them in x-y coordinates. To do this we will use the `.hist()` method of `plt` and pass in the `maxtemp` variable as our `x=` parameter.\n",
    "\n",
    "To show plots we need to use `plt.show()` after creating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=df['maxtemp'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a nice and informative plot as it tells us upon first inspection that our data is normally distributed (i.e. it has an reliable average value;ore on this on lesson 5). What would make it even better is, a bit more information. Let's do that by passing in a few additional parameters to the hist and the plt functions.\n",
    "\n",
    "- `bins=` --> will tell matplotlib how many bars we want in the distribution of the data\n",
    "- `color=` --> allows us to change the color of the bars. You can find more about the [colors here](https://matplotlib.org/2.0.2/api/colors_api.html)\n",
    "- `endgecolor=` --> allows us to give color to the edge of a figure\n",
    "- `plt.title()` --> allows us to add a title to our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=df['maxtemp'], color='green', bins=40, edgecolor='white')\n",
    "plt.title('Maximum Temperature - 2007-2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also show different distributions at the same time by adding another `plt.hist()` before our `plt.show()` function. For example, let's look at the `mintemp` as well, but this time, let's change a few other parameters inside our plots.\n",
    "\n",
    "- `histtype=` --> provides us with different options on how to draw the histogram; the default is bar\n",
    "- `alpha=` --> allows us to add transparency to our plots\n",
    "- `fontdict=` --> we can pass in a dictionary containing the font for the title as well as a few other functionalities\n",
    "- `plt.xlabel()` --> allows us to add a label to the x axis\n",
    "- `plt.ylabel()` --> allows us to add a label to the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=df['maxtemp'], color='red', bins=50, histtype='stepfilled', alpha=0.4)\n",
    "plt.hist(x=df['mintemp'], color='cyan', bins=50, histtype='stepfilled', alpha=0.5)\n",
    "plt.title('Maximum & Minimum Temperature - 2007-2017', fontdict={'fontsize': 15})\n",
    "plt.xlabel('Temperature Distribution in Celcius')\n",
    "plt.ylabel('Temperature Frequency in 10 Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would like to see the density of each variable as opposed to the counts, we could pass in the `density=True` parameter. The density tells us what percentage of the data can be found at a specific temperature. For example, in the plot below we can see that in the `mintemp` variable we can see that for those 10 years, 60% of the time the mintemperature around Australia was around 10 degrees Celcius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=df['maxtemp'], color='red', bins=50, histtype='stepfilled', alpha=0.4, density=True)\n",
    "plt.hist(x=df['mintemp'], color='blue', bins=50, histtype='stepfilled', alpha=0.5, density=True)\n",
    "plt.title('Maximum & Minimum Temperature - 2007-2017', fontdict={'fontsize': 15})\n",
    "plt.xlabel('Temperature Distribution in Celcius')\n",
    "plt.ylabel('Temperature Frequency in 10 Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise more data. Now, we will move on to scatter plots, which are a type of visualisation that allows us to see relationships between different variables. There are two ways of creating scatter plots in matplolib. One is by using `plt.scatter()` and the other is by using `plt.plot(x=, y=, 'o')`. Notice how the third parameter has no name, this is the way matplotlib deals with this functionality for plots. We will use `scatter` as it is more straightforward but the latter is more robust to large datasets, meaning, it handles large amounts of data better. This is because it does less computation behind the scenes. You can read more about this in the matplotlib's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df['rainfall'], y=df['temp3pm'], marker='o', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customise our plots again with a few additional parameters and plt calls. We can also apply a new plotting style to our visualisations.\n",
    "\n",
    "- `marker=` --> picks the appropriate marker for our plot\n",
    "- `s=` --> adjusts the size of our marker\n",
    "- `plt.xlim()` --> allows us to customise the start and end points of the x axis. Notice how we are cutting some outliers below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# our plot\n",
    "plt.scatter(x=df['rainfall'], y=df['temp3pm'], marker='.', color='red', alpha=0.7, s=1.5)\n",
    "plt.xlim(0, 250)\n",
    "\n",
    "# annotations\n",
    "plt.title('Relationship Between Rain and Temperature at 3 pm from 2007-2017', fontdict={'fontsize': 10})\n",
    "plt.xlabel('Rainfall')\n",
    "plt.ylabel('Temperature Frequency at 3 pm')\n",
    "\n",
    "# we can also save our figs with this command, dpi= manages the quality (the higher the better)\n",
    "plt.savefig('my_cool_plot.png', dpi=500)\n",
    "\n",
    "# showing our figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach - Object Oriented Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second we will be assigning two objects that come out of the `plt.subplots()` method, a figure and an axes object. The figure controls everything related to size, saving, and manipulating the figure as a whole. The axes object allows us to control everything that goes into the figure such as the type of plot, how many, and which customisations we would like to see. You can think of the `fig` object as our container and the `ax` object as our data.\n",
    "\n",
    "Some of the commands in the previous approach have a slightly different call. Here are some of them:\n",
    "\n",
    "- `plt.legend()` --> `ax.legend()`\n",
    "- `plt.xlabel()` --> `ax.set_xlabel()`\n",
    "- `plt.ylabel()` --> `ax.set_ylabel()`\n",
    "- `plt.xlim()` --> `ax.set_xlim()`\n",
    "- `plt.ylim()` --> `ax.set_ylim()`\n",
    "- `plt.title()` --> `ax.set_title()`\n",
    "\n",
    "To customise our charts we can take advantage of the following parameters and pass them through our `ax` object.\n",
    "\n",
    "- `x=`\n",
    "- `y=`\n",
    "- `data=`\n",
    "- `color=`\n",
    "- `linestyle=`\n",
    "- `linewidth=`\n",
    "- `marker=`\n",
    "- `markersize=`\n",
    "- `markeredgecolor=`\n",
    "- `markerfacewidth=`\n",
    "- `markeredgewidth=`\n",
    "- `alpha=`\n",
    "\n",
    "A very useful way of visualising data is by aggregating it first. It doesn't change the parameters or the way we would create a visualisation with the regular dataset, but rather it helps us create very concise plots. Let's create a quick pivot table to see how the humidity changed throughout the day during the years for which we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_by_year = df.pivot_table(\n",
    "    index='year',\n",
    "    values=['humidity9am', 'humidity3pm'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "hum_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fig and ax objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# add data to the ax object\n",
    "ax.plot(hum_by_year.index, hum_by_year['humidity9am'])\n",
    "\n",
    "# show the fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is very nice and informative. Let's make it even better by adding the 3pm humidity variable to another `ax` object and by adding some labels and a marker for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fig and ax objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# add the 9am data and label to the ax object\n",
    "ax.plot(hum_by_year.index, hum_by_year['humidity9am'], label='Humidity 9am', marker='o')\n",
    "# add the 3pm data and label to the ax object\n",
    "ax.plot(hum_by_year.index, hum_by_year['humidity3pm'], label='Humidity 3pm', marker='v')\n",
    "\n",
    "ax.set_xlabel(\"Years\")\n",
    "ax.set_ylabel(\"Average Humidity\")\n",
    "\n",
    "# show the legend with labels\n",
    "plt.legend()\n",
    "# show the fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we would like to show the distributions of our 4 temperature variables. If we were to do this in one single plot, this could result in a very busy plot that might be not be as informative as we want it to. Let's see what this would look like in one chart and then explore options for multiple plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Create the fig and ax objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# add the 9am data and label to the ax object\n",
    "ax.hist(df['mintemp'], bins=55, label='Max Temp')\n",
    "ax.hist(df['maxtemp'], bins=55, label='Min Temp')\n",
    "ax.hist(df['temp9am'], bins=55, label='Temp 9am')\n",
    "ax.hist(df['temp3pm'], bins=55, label='Temp 3pm')\n",
    "\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "# show the legend with labels\n",
    "plt.legend()\n",
    "# show the fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super informative right? What we could do insted is to create four figures showing the distributions of temperatures across our dataset. To do this, we will pass in the shape of our figure to our `.subplots()` method, and select the index of each figure as if we were doing so in a matrix, with slicers in our `ax` parameters. We will also add to our `.subplots()` method, the `sharey=True` to let matplotlib know that we want our for figures to share the same y axis. The next step we will take is to add a `figsize=` parameter to our figure to tinker with the size of the end result.\n",
    "\n",
    "We will add color the same way we have done so above and we will make our figures a bit more transparent with the `alpha=` parameter of our `ax` objects.\n",
    "\n",
    "Lastly, we will set the labels of each figure's x axis, and only the y axis with values in it. We will finish by adding a `fig.savefig()` to save our figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartoon style for fun : )\n",
    "# plt.xkcd()\n",
    "\n",
    "# Create the fig and ax objects\n",
    "fig, ax = plt.subplots(2, 2, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# add the 9am data and label to the ax object\n",
    "ax[0, 0].hist(df['mintemp'], bins=55, color='blue', alpha=0.5)\n",
    "ax[0, 1].hist(df['maxtemp'], bins=55, color='red', alpha=0.5)\n",
    "ax[1, 0].hist(df['temp9am'], bins=55, color='cyan', alpha=0.5)\n",
    "ax[1, 1].hist(df['temp3pm'], bins=55, color='green', alpha=0.5)\n",
    "\n",
    "\n",
    "# ax.set(title=\"Distributions of Temperatures between 2007-2017\")\n",
    "ax[0, 0].set_xlabel(\"Min Temperature\")\n",
    "ax[0, 1].set_xlabel(\"Max Temperature\")\n",
    "ax[1, 0].set_xlabel(\"Temp at 9am\")\n",
    "ax[1, 1].set_xlabel(\"Temp at 3pm\")\n",
    "ax[0, 0].set_ylabel(\"Frequency\")\n",
    "ax[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "fig.savefig('temp_plots.png')\n",
    "\n",
    "# show the fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our introduction to matplotlib and we will now move on to use its cousin, seaborn. We will continue to use both tools for the remainder of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Introduction to Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seaborn](https://banner2.cleanpng.com/20181109/pi/kisspng-logo-image-python-font-product-spread-networks-and-seaborn-team-up-to-provide-sea-5be5f5e0aa1a53.8473640515417973446968.jpg)\n",
    "\n",
    "**Source:** https://www.cleanpng.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is a statistical data visualisation library built on top of matplotlib. This means that, a lot of the components of seaborn will be relatively familiar to you since you already went over some of the components of matplotlib in the previous section.\n",
    "\n",
    "Some of the characteristics of seaborn are:\n",
    "\n",
    "- Easier than matplotlib\n",
    "- Beautiful visualisations\n",
    "- Integrates very well with pandas\n",
    "- Has a nickname too, `sns` :) \n",
    "- To use it in a session you have to import matplotlib as well\n",
    "- `plt.show()` has to be used as well to render the plots\n",
    "- Seaborn works better with tidy datasets\n",
    "\n",
    "\n",
    "Let's begin by importing seaborn, matplotlib, pandas, and numpy with their industry alias, and by using our matplotlib magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load our ready-to-go dataset again\n",
    "df = pd.read_csv('weather_ready.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method we will use is the `.countplot()` method. This method allows us to count the data in a categorical variable and visualise it as a bar plot. The parameters we will need are:\n",
    "\n",
    "- `x=` --> this is our variable of interest\n",
    "- `data=` --> our dataframe with the x variable above\n",
    "- `plt.xticks(rotation=45)` --> allows us to rotate the labels of our columns when they don't fit\n",
    "\n",
    "Notice how seaborn uses the name of our variable as the x axis label and the count word for our y axis. As with matplotlib, we call our `plt.show()` method at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='windgustdir', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Where does the wind come from most of the time?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice or filter our dataframe as we pass it through the `.countplot()` method. Here we only want to see the rainy days in Sydney for the year 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='raintoday', data=df[(df['location'] == 'Sydney') & (df['year'] == 2016)])\n",
    "plt.xlabel(\"Did it Rain Today?\")\n",
    "plt.title(\"Did it rain a lot in Sydney in 2016?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualise categorical data with seaborn is by using the `.catplot()` method. This time we need to make sure we pass in the parameter `kind=` to our `sns` call and specify which measure we would like to see.\n",
    "\n",
    "Let's create a subset of the data for the year 2016 and visualise the `raintomorrow` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = df[df['year'] == 2016].copy()\n",
    "df_2016.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='raintomorrow', data=df_2016, kind='count')\n",
    "plt.xlabel(\"Did it Rain Tomorrow?\")\n",
    "plt.title(\"Did it rain a lot in 2016?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass in several categorical variables to the `.catplot()` or even combine quantitative variable with categorical ones. In the example below we combine the `rainfall` and `day_of_week` variables to show an interesting comparison. The little bars at the top are the confidence intervals, which we will discuss in more depth on lesson 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='day_of_week', y='rainfall', data=df_2016, kind='bar')\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Rain Fall (inches)\")\n",
    "plt.title(\"Amount of rain in inches and per day in 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots are another great tool to show statistical distributions in the data. The box and whiskers hold 99% of the data. The top and bottom lines are the max and the min values, respectively, or the variable shown. The top and bottom of the box are the 75th and 25th percentiles, respectively, and the middle line is the median of the distribution.\n",
    "\n",
    "Any points above the max and min of the distribution is consider an outlier. An extreme number that could be due to many things, such as errors of measurement, actual observation, wrong missing value, and a few others. We will talk more about outliers in lesson 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='day_of_week', y='maxtemp', data=df_2016, kind='box')\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Max Temp\")\n",
    "plt.title(\"Distribution of the Max Temp in the weekdays of 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can arange a palette of colors to match a category in a variable, pass it into our `sns` plot and get a nice color split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_colors = {'weekend': \"green\", 'week_day': \"blue\"}\n",
    "\n",
    "sns.countplot(x='raintoday', data=df[(df['location'] == 'Sydney') & (df['year'] == 2016)],\n",
    "              hue='week_or_end', palette=palette_colors)\n",
    "\n",
    "plt.title(\"Did it rain a lot in Sydney in 2016?\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very useful tool in seaborn is the `.scatterplot()` method. It is very versitile and straightforward to use and it allows us to see the correlation between two quantitative variables. To use it we pass in:\n",
    "\n",
    "- `x=` --> this is one variable of interest\n",
    "- `y=` --> this is another variable of interest\n",
    "- `data=` --> our dataframe with the x and y variable above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.scatterplot(x='maxtemp', y='rainfall', data=df)\n",
    "plt.title(\"Are rainy days correlated with high temperatures?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the plots in the data by adding the name of a categorical variable to the `hue=` parameter. This will split the data into a number of colors equal to the amount of categories in the variable. This is a great feature for scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='humidity9am', y='temp3pm', data=df.sample(frac=.05), hue='raintomorrow')\n",
    "plt.xlabel(\"Humidity at 9am\")\n",
    "plt.ylabel(\"Humidity at 3pm\")\n",
    "plt.title(\"If it is humid in the mornig will it be humid in the afternoon?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to display relational plots is the `.relplot()`. Like the `.catplot()` method, it requires that we pass in a `kind=` parameter with the word `'scatter'` to it, and it will provide us with the same result but with the added benefit of having more flexibility. For example, if we would like to split a scatterplot by the categories of a variable, we could do so with `col=` parameter and by passing the categorical variable to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='rainfall', \n",
    "            y='windspeed9am', \n",
    "            data=df_2016, \n",
    "            kind='scatter',\n",
    "            col='week_or_end')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add even more by using the `row=` parameter and another categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='rainfall', \n",
    "            y='windspeed9am', \n",
    "            data=df_2016, \n",
    "            kind='scatter',\n",
    "            col='week_or_end',\n",
    "            row='raintomorrow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn also gives us the option to enlarge quantitative variable by a category variable using the `size=` parameter. Let's create a dataframe less rain outliers and see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_no_outliers = df_2016[df_2016['rainfall'] < 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='rainfall', \n",
    "            y='maxtemp', \n",
    "            data=df_2016_no_outliers.sample(frac=0.2), \n",
    "            kind='scatter',\n",
    "            hue='weekday',\n",
    "            size='weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also create trend lines with `.relplot()` by specifying the parameter `kind=` to `'line'`. By default `sns` will show the confidence interval of the values in our trend. This means that we can be 95% confident the average rain in a given month will be within the shaded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='month', y='rainfall', data=df_2016, kind='line')\n",
    "plt.xlabel('Months of 2016')\n",
    "plt.ylabel('Rain in inches')\n",
    "plt.title(\"Precipitation Monthly Trend in 2016\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding a `style=` and `hue=` parameter with a categorical variable, we can show even more patterns in the data. Setting `ci=` to False will remove the confidence interval lines that are added to line plots by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='month', y='rainfall', data=df_2016, kind='line', style='week_or_end', hue='week_or_end', markers=True, ci=False)\n",
    "plt.xlabel('Months of 2016')\n",
    "plt.ylabel('Rain in inches')\n",
    "plt.title(\"Precipitation Monthly Trend by Week or W-End in 2016\")\n",
    "plt.savefig(\"more_trends\", dpi=350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Introduction to Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bokeh](https://repository-images.githubusercontent.com/3834332/3a947e00-a987-11e9-94f8-94b8136aaf78)\n",
    "\n",
    "**Source:** https://github.com/bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh is one of the best data visualisation tools in the whole Python data analytics stack, specially when it comes to creating interactive plots. It allows us to create beautiful visualisations that can be displayed as interative graphs in the browser. This is short introduction to some of the many useful stories one can tell with bokeh.\n",
    "\n",
    "The first concept we need to learn about are Glyphs. Glyphs are visual shapes that can be drawn to screen the same way we have been doing it with matplotlib and seaborn. Glyphs are like visual shapes such as circles, squares, triangles, coordinates, size, color, transparency, and many others.\n",
    "\n",
    "To show the output of a graph in the browser we need to load, and run once, the `output_file` command. If we pass in a name as a string and an `.html` extension. We can save our files as a website. The notebook equivalent would be `output_notebook`. It also only needs to run once, but please keep in mind that crating and displaying the visualisations in our notebooks will make them (momentarily) heavier and slower. For that reason, it might be best to show them in the browser.\n",
    "\n",
    "We will also make use of, just like in matplotlib, a `figure` object which we can import from bokeh's plotting module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_ready.csv', parse_dates=['date'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your figure and add it to a variable\n",
    "plot = figure(plot_width=400, tools='pan, box_zoom')\n",
    "\n",
    "# use your Glyph method of choice and pass in vars\n",
    "plot.circle(df['rainfall'], df['mintemp'])\n",
    "output_file('circle.html') # will save the output as an html file\n",
    "\n",
    "# show your figure\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are more parameters for you to experiment with\n",
    "p = figure()\n",
    "\n",
    "# and you can be explicit about your variables\n",
    "# and the size of the circles\n",
    "p.circle(x=df['rainfall'], y=df['humidity9am'], size=4)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of all of the markers one can use in bokeh. We will mostly use circle for this session.\n",
    "\n",
    "- asterisk\n",
    "- circle\n",
    "- circle_cross\n",
    "- circle_x\n",
    "- cross\n",
    "- diamond\n",
    "- diamond_cross\n",
    "- inverted_triangle\n",
    "- square\n",
    "- square_cross\n",
    "- square_x\n",
    "- triangle\n",
    "- x\n",
    "- lines\n",
    "\n",
    "Let's bring back our pivot table from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_by_year = df.pivot_table(\n",
    "    index='year',\n",
    "    values=['humidity9am', 'humidity3pm'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "hum_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another figure\n",
    "plot_line = figure()\n",
    "\n",
    "# lines are great for showing trends\n",
    "plot_line.line(x=hum_by_year.index, y=hum_by_year['humidity3pm'], line_width=2)\n",
    "\n",
    "# and we can combine them in the sample figure with other markers, change the size, and add color to the marker\n",
    "plot_line.circle(x=hum_by_year.index, y=hum_by_year['humidity3pm'], fill_color='white', size=10)\n",
    "show(plot_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2 = figure()\n",
    "plot2.circle(df['rainfall'], df['maxtemp'], size=4)\n",
    "show(plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add different tools to your figures. For example, box_selext and lasso_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tools of a figure add interactivity to a plot\n",
    "plot3 = figure(tools='box_select, lasso_select') # rectangular vs free form selection\n",
    "\n",
    "plot3.circle(df['rainfall'], df['windgustspeed'],\n",
    "            selection_color='red', # the color for our selection with lasso or box tools\n",
    "            nonselection_fill_alpha=0.2, # transparency of the non-selected data\n",
    "            nonselection_fill_color='grey') # color of the non-selected data\n",
    "\n",
    "# show the plot\n",
    "show(plot3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hover tool allows us to hover over the chart while selecting data\n",
    "\n",
    "hover = HoverTool(tooltips=None, mode='hline')\n",
    "\n",
    "# the crosshair option gives us a cross to hover with\n",
    "plot4 = figure(tools=[hover, 'crosshair'])\n",
    "\n",
    "# we can add color to the hover tool with hover_color parameter\n",
    "plot4.circle(df['rainfall'], df['humidity9am'],\n",
    "            size=5, hover_color='red')\n",
    "\n",
    "show(plot4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import CategoricalColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tool allows us to map specific colors to specific categories within a variable\n",
    "mapper = CategoricalColorMapper(\n",
    "    factors=['first_Q', 'second_Q', 'third_Q', 'fourth_Q'],\n",
    "    palette=['bisque', 'rosybrown', 'chocolate', 'maroon']\n",
    ")\n",
    "\n",
    "# labels can be added within the figure parameter\n",
    "plot5 = figure(x_axis_label='rainfall',\n",
    "              y_axis_label='mintemp'\n",
    "              )\n",
    "\n",
    "plot5.circle('rainfall', 'mintemp',\n",
    "            size=7, source=df,\n",
    "            color={'field':'qtr_cate', # pass in the color as a dictionary and specify the field first, e.g. our qrt_cate variable\n",
    "                  'transform': mapper}, # assign the colors with transform param\n",
    "            legend='qtr_cate' # add your legend for the categories\n",
    "           )\n",
    "\n",
    "# move the legend to a convenient spot\n",
    "plot5.legend.location = 'top_right'\n",
    "\n",
    "show(plot5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows and Columns of Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import row, column, gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows of our previous plots\n",
    "layout = row(plot5, plot4, plot2, plot_line) \n",
    "# output_file('new.html')\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of our previous plots\n",
    "layout = column(plot5, plot4, plot2, plot_line) \n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns of our previous plots\n",
    "layout = row(column(plot4, plot2), plot5, plot_line) \n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as row-col but in one place\n",
    "layout = gridplot([[plot5, plot4], [plot_line, plot2]],\n",
    "                  toolbar_location=None) # above, below, left, or right. now it is Nowhere\n",
    "\n",
    "# output_file('circle.html')\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabbed layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.widgets import Tabs, Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create the panels that will go in the tabs, add a title to each\n",
    "\n",
    "first = Panel(child=row(plot4, plot_line), title='first')\n",
    "second = Panel(child=row(plot5, plot3), title='second')\n",
    "\n",
    "# pass the panels in a list to the tabs= parameter of the Tabs object\n",
    "tabs = Tabs(tabs=[first, second])\n",
    "\n",
    "# show the tabs :)\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Data Visualisation Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a non-exhaustive list of things to keep in mind when creating visualisations with data. This list was created by Nicolas P. Rougier and it can be found in his article called, \"_[Ten Simple Rules for Better Figures](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003833)_\".\n",
    "\n",
    "- Rule 1: Know Your Audience\n",
    "- Rule 2: Identify Your Message\n",
    "- Rule 3: Adapt the Figure to the Support Medium\n",
    "- Rule 4: Captions Are Not Optional\n",
    "- Rule 5: Do Not Trust the Defaults\n",
    "- Rule 6: Use Color Effectively\n",
    "- Rule 7: Do Not Mislead the Reader\n",
    "- Rule 8: Avoid “Chartjunk”\n",
    "- Rule 10: Get the Right Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned a lot in this lesson so let's summarise a few of the most important concepts.\n",
    "\n",
    "- Data visualisation is great for exploring our datasets\n",
    "- It allows us to see interesting patterns in the data that might not be visible upon first inspection\n",
    "- Before any data visualisation takes place, we need to clean and prepare our dataset\n",
    "- Satatic graphs are great for telling a story from one angle\n",
    "- Interactive visualisations are great for involving the audience with the message we are trying to convey\n",
    "- Python has excelent tools for visualisation such as matplotlib, seaborn, and bokeh\n",
    "\n",
    "To feed your curiosity.\n",
    "\n",
    "### Python\n",
    "- [holoviews](http://holoviews.org/index.html) --> similar to bokeh but easier to use\n",
    "- [datashader](https://datashader.org/) --> great for massive datasets\n",
    "- [Altair](https://altair-viz.github.io/index.html) --> beatiful data visualisation library based on D3 and Vega\n",
    "\n",
    "### Other tools\n",
    "- [D3js](https://d3js.org/) --> state of the art JavaScript tool for data visualisation\n",
    "- [Vega Lite](https://vega.github.io/vega-lite/) --> stunning data visualisation tool (very involved)\n",
    "- [ggplot2](https://ggplot2.tidyverse.org/) (R's most famous visualisation library)\n",
    "\n",
    "### Favourite Visualisation Websites\n",
    "\n",
    "- [FlowingData](flowingdata.com)\n",
    "- [Information is Beautiful](https://www.informationisbeautifulawards.com/)\n",
    "- [eagereyes](https://eagereyes.org/)\n",
    "\n",
    "### How to...? Websites\n",
    "- [Python Graph Gallery](https://python-graph-gallery.com/)\n",
    "- [Drawing from Data](https://www.drawingfromdata.com)\n",
    "\n",
    "### Data Visualisation Experts You Might Want to Know About\n",
    "- [Jan Willem](http://tulpinteractive.com/)\n",
    "- [Christian Laesser](https://christianlaesser.com/)\n",
    "- [Andy Kirk](https://www.visualisingdata.com/)\n",
    "- [Eduard Tufte](https://www.edwardtufte.com/tufte/)\n",
    "- [Alberto Cairo](http://albertocairo.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wickham, Hadley. _“Tidy Data.”_ Journal of Statistical Software, vol. 59, no. 10, 2014, doi:10.18637/jss.v059.i10.\n",
    "\n",
    "Kirk, Andy. _Data Visualisation: a Handbook for Data Driven Design_. SAGE Publications Ltd, 2019.\n",
    "\n",
    "VanderPlas, Jake. _Python Data Science Handbook_. O'Reilly, 2017.\n",
    "\n",
    "McKinney, Wes. _Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython_. OReilly, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would really appreciate it if you could please provide us with your feedback from this session by filling a couple of question.\n",
    "\n",
    "> ## [Survey](https://docs.google.com/forms/d/e/1FAIpQLScYo-eXeALcrqmcGq9u9PqoJJV-T-y13jGunL1bvqPKAGdc6Q/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
