{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stats](https://infobeautiful4.s3.amazonaws.com/2019/02/static_winner.png)  \n",
    "**Source:** Information is Beautiful 2018 Awards - Data Visualisation Created by [Dimiter Toshkov](http://dimiter.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Recap of lesson 4  \n",
    "II. Learning Outcomes  \n",
    "\n",
    "\n",
    "__Outline for the session:__\n",
    "\n",
    "1. Introduction to Statitics\n",
    "2. Collecting\n",
    "3. Data Cleaning and Preparation\n",
    "    - Load\n",
    "    - Inspect\n",
    "    - Clean & Prepare\n",
    "4. Describing Data\n",
    "5. Summary\n",
    "6. References\n",
    "7. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Recap of lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 4 we covered the following topics/concepts.\n",
    "\n",
    "- Data visualisation is great for exploring our datasets\n",
    "- It allows us to see interesting patterns in the data that might not be visible upon first inspection\n",
    "- Before any data visualisation takes place, we need to clean and prepare our dataset\n",
    "- Satatic graphs are great for telling a story from one angle\n",
    "- Interactive visualisations are great for involving the audience with the message we are trying to convey\n",
    "- Python has excelent tools for visualisation such as matplotlib, seaborn, and bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the lesson you will have learned:\n",
    "\n",
    "1. What is Statistics?\n",
    "2. How does data collection work?\n",
    "3. How to describe data effectively to convey facts.\n",
    "4. How to explore your data more effectively with pivot tables and groupby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics is one of the most widely used areas of mathematics. It helps us mere mortals collect, describe, and make inferences about natural and unnatural phenomena using data. It allows us to approximate answers to questions that were thought to be impossible to answer. In addition, a large number of scientific advancements have used this area of mathematics to quantify, test, validate, and improve our answers to problems with a wide range of complexities. For example, Albert Einstein and his groundbreaking theories of Special and General Relativity used statistics to approximate the movement of the sun in relation to that of other bodies of masses around our galaxy. To provide you with a more formal definition of statistics, the UCI Department of statistics has defined it as:\n",
    "\n",
    "\"...the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data.\" Empirical here means based on observations and verifiable evidence as opposed to logic and deduction.\n",
    "\n",
    "Now that we know what \"_Statistics_\" is, let's define what \"_A Statistic_\" is. A Statistic is a piece of evidence or a fact obtained via a calculation performed on a sample of data. When you hear comments on the radio or read news statements that begin with, \"the average rugby player...\", \"the average Australian now days...\", etc., you are essentially being given \"_A Statistic_\". A fact compiled from a sample of data.\n",
    "\n",
    "__What is a Sample?__ A sample is a fraction of the population. For example, the students in this course, are a sample of the overall number of students in Australia. In this case, the number of students in Australia would be considered the total population of students, and the students of this course the sample.\n",
    "\n",
    "__What is the Population?__ The population is a set of data with some similarity, and it is the whole from where a sample is drawn from. Think about a cake sliced into pieces, the full cake is the population and a slice of it is the sample. Mickey can show us a good representation of this example.\n",
    "\n",
    "![cake](https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif)\n",
    "\n",
    "Going back to the example above, all students in Australia would be the population and the defining characteristic of this population is that they are all active students. If we needed, for example, the address of every one of these students but were unable to get 10 of them, we would call the set of students belonging to that characteristic, the address, a sample as it is a fraction of the full set. Another way to put it would be `Population - 10 = Sample`.\n",
    "\n",
    "## What can we do with Statistics?\n",
    "\n",
    "- We can learn from information.\n",
    "- Draw conclusions based on evidence.\n",
    "- Simulate the real world.\n",
    "- Mine massive amounts of data.\n",
    "- Spot liars.\n",
    "- Separate correlations from causation.\n",
    "\n",
    "> “The best thing about being a statistician is that you get to play in everyone else’s backyard.” ~ John Tukey\n",
    "\n",
    "## What should we not do with Statistics?\n",
    "\n",
    "- Lie/deceive others.\n",
    "\n",
    "![lie_stats](https://herdingcats.typepad.com/.a/6a00d8341ca4d953ef01a511d248cc970c-pi)  \n",
    "**Source:** \"_How to Lie with Statistics_\" by Darrell Huff, 1954\n",
    "\n",
    "- Assume that because two things seem to be related to each other, one causes the other. A famous example of this was given by Franz H. Messerli, MD in 2012. He found a surprisingly large positive correlation between the chocolate consumption of a country and the amount Nobel Laureates that same country has produced. See below.\n",
    "\n",
    "![chocolate](https://i.insider.com/5353e29b6da8115322dd4816?width=1200)  \n",
    "\n",
    "- Overgeneralise results. Just because an analysis of a group of consumers found that 60% of them buy goods before a holiday does not mean that 60% of all humans do.\n",
    "- Avoid falling pray of confusing annecdotal evidence vs real evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to have a project or idea you wish to explore before moving onto the data collection part. The reason behind this is that once you have context on what you would like to analyse, you will be able to search for that information more efficiently. For example, if you need to study the profitability of a movie theatre in a given year, you would need data about sale and airing times for the movies shown within your specific timeframe. If you would like to evaluate peoples' sentiment towards a political candidate on social media, you would need social media data and might benefit from looking in Twitter first.\n",
    "\n",
    "Whether you have a question to answer or not, collecting data becomes the most important piece of the statistics puzzle. Especially because after all, without anything to analyse, we would be left with theoretical work only. Which is very important but also relies on proving such theories with real examples.\n",
    "\n",
    "Collecting data can be as easy as clicking a button and downloading a table full of content that can be opened in excel (structured data), or as difficult as crawling over many websites and getting (unstructured) data to then formating it into a more usable structure. Here are two examples of how we encounter data:\n",
    "\n",
    "__Structured__\n",
    "\n",
    "|    Name   |   Income  |  Age  | Favourite Brand |\n",
    "|-----------|-----------|-------|-----------------|\n",
    "|  Abraham  |   70,000  |   25  |   Nike          |\n",
    "|  Lisa     |  110,000  |   31  |   Apple         |\n",
    "|  Mark     |   80,000  |   24  |   Garmin        |\n",
    "|  Penelope |   55,000  |   49  |   Microsoft     |\n",
    "|  Michaela |   74,000  |   51  |   ZARA          |\n",
    "|  Nicholas |   61,000  |   36  |   Adiddas       |\n",
    "\n",
    "__Unstructured__\n",
    "\n",
    "<img src=\"https://www.lubys.com/img/recipes/L929_RecipeCards_7x5_cfs_front.jpg\" alt=\"recipe\" width=\"600\"/>  \n",
    "**Source:** https://www.lubys.com/recipes\n",
    "\n",
    "Notice that in the example above, the structured data resembles what we are used to seeing when we use or think of data. On the other hand, we might not be used to thinking that recipes from a restaurant, emails, poems, social media posts, all qualify as data. The most notable differences being the immediate usability of it and the unclear picture as to how to set it into a structured format.\n",
    "\n",
    "As analysts we can collect data through surveys, purchasing history (these are often stored automatically into a database like SQL or Microsoft Access), by tracking visits to a website, and many more. The information we get in a dataset, the data in its variables (or in excel lingo, the columns), is often represented in one of two ways: as categorical and quantitative information.\n",
    "\n",
    "Categorical data represent groups or specific characteristics of the data. For example, a column called gender that contains whether a survey participant was a male, female, or other, will count as a categorical variable. This type of classification, gender, counts as a specific type of categorical data called nominal.\n",
    "\n",
    "Nominal data is the type of data that can be classified into groups but with no particular order to them. In the example of gender, we cannot say that female is greater than male or vice-versa. This takes us to our next type of categorical data, ordinal.\n",
    "\n",
    "Ordinal data are the type of data where order matters. Imagine you have a dataset with formula one drivers in the rows and all of the characteristics regarding their professional careers in the columns. If we were to have a variable for every race in which they competed, plus their placement within each race, keeping the order in which they crossed the finish line would be crucial for our dataset to be meaningful. Especially since first has a higher value than second, and of course, last. Another example of ordinal data is the temperature of a meal ordered at a restaurant. If we order a stake medium-well, we would expect to get the stake at a temperature within that range. In this scenario, order from hot to cold matters.\n",
    "\n",
    "Quantitative data, on the other hand, represents numerical data that can be quantified, summarized, averaged, and visualized, usually in more ways than categorical data can. Quantitative data can be income, revenue, stock prices, miles, weight, and height, and many more. \n",
    "\n",
    "It is important to notice that categorical data, whether nominal or ordinal, can still be collected in a numerical format without it being considered quantitative. Say we collect data for the level of spiciness of different meals at a restaurant and assign these values the numbers 3 for hot, 2 for warm, and 1 for cold, they would still be considered categorical data even though it would now be represented by a numerical value. One reason being that the quantities of this variable are finite.\n",
    "\n",
    "## Statistical Data Collection\n",
    "\n",
    "![stat_data](https://www.karmelsoft.com/wp-content/uploads/2019/10/data-collection-ideas.jpg)  \n",
    "**Source:** https://www.karmelsoft.com/\n",
    "\n",
    "When we collect data specifically for statistical purposes we want to gain knowledge or learn about something that is important to us. There are two ways in which statistics concerns itself when it comes to data collection, and those are through observation and experimentation.\n",
    "\n",
    "An important point to keep in mind is that, although we have an idea of what the population we're interested in might look like or what the magnitude of it might be, we would never collect enough data to make a 100% prediction based on it. With that said though, there are many sampling methods that do allow us to get pretty close to our desired outcome. Let's first define observational and experimental studies.\n",
    "\n",
    "**Sample Study:** a study where we try to estimate the true value of a parameter based on a sample of the real population. A parameter in this case would be a piece of information such as the mean, median, and mode in a sample regarding our question of interest. If you would like to know how many people per country, on average believe in global warming, asking everyone in the planet would be impossible. But by asking a random sample of people from each country, we could approximate the true average.\n",
    "\n",
    "**Observational Study:** In these kinds of studies, we are interested in a particular subsection of the population and we would like to determine what causes what without intervening with the sample being studied. For example, say we want to study the relatioship between smoking and drinking and we whether one causes the other or vice-versa. We would observe people in their natural environment and collect data about what happens when these actions take place.\n",
    "\n",
    "We also need to be cautious when upon first inspection all of our assumptions seem to match. A high correlation between both, smoking and drinking does not mean that one causes the other. It could be that there is a **confounding variable** behind the scenes controling the outcomes. For example, drinking and smoking could be due to being in a circle of friends where everyone smokes. It could also be a habbit triggered by the cue, \"out with friends socialising.\" Confounding variables, or for that matter, the real causes of a phenomena, are often hard to pick appart but not impossible.\n",
    "\n",
    "**Experimental:** Experiments are the toughest to due but are also the ones that can get you the cosest to causality. With experiments we are in search of what causes what and even go to great lenghts to pick random sample of people, then assign them to random groups, and induce the variable of interest to half of that sample and a placebo to the other. These two groups are also called, control and treatment group. If the size of the difference between the two groups is large or significant enough once the experiment ends, it can be possible to determine causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://photos.nomadicnotes.com/img/s9/v96/p1664606808-4.jpg\" alt=\"melbourne\" width=\"600\"/>  \n",
    "\n",
    "**Source:** https://www.nomadicnotes.com/airbnb-apartment-southbank-melbourne-australia/  \n",
    "\n",
    "The dataset we will be using contains information about Airbnb's hosts and their places in Melbourne, VIC, AU. The data was scraped from Airbnb's website by a tool called [Inside Airbnb](http://insideairbnb.com/). Their website is fascinating and contains information not only on Melbourne but also on most of the cities around the world where Airbnb operates.\n",
    "\n",
    "Inside Airbnb periodically scrapes Airbnb's data, and we will be working with a Melbourne sample that was last scraped in 2018 that was posted in Kaggle. We will not have a list of the variables and what they represent since there are too many in the dataset for that. Instead, to learn more about the dataset you can navigate to the Kaggle post through [this link](https://www.kaggle.com/tylerx/melbourne-airbnb-open-data?select=cleansed_listings_dec18.csv), go to Airbnb and search for listings in Melbourne to see the information shown, or you can find more information about the datasets at [Inside Airbnb Get the Data Section](http://insideairbnb.com/get-the-data.html).\n",
    "\n",
    "Let us now load, inspect, and clean our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for windows users in CMD\n",
    "!type datasets\\files\\airbnb_melbourne.csv | more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac users or windows users with Git Bash\n",
    "!head -n 5 ../datasets/files/airbnb_melbourne.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/files/airbnb_melbourne.csv', \n",
    "                 parse_dates=['last_scraped', 'host_since', \n",
    "                              'calendar_last_scraped', 'first_review', \n",
    "                              'last_review'], \n",
    "                 low_memory=False) # the dataset is quite large so we set the low_memory setting to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the memory usage at the very bottom\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = ((df.isna().sum() / df.shape[0]) * 100)\n",
    "missing_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Clean & Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set a threshold for missing values and get rid of anything above that threshold using the series we created in the previous section. We'll drop all columns where the missing values amount to more than 25%. For this, we will create a mask and select the indeces of the columns array and filters the ones we would like to drop. We will pass that into pandas `.drop()` method and then specify that want to drop columns and not rows with the `axis=1` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = missing_pct[missing_pct > 25].index # our boolean array\n",
    "df.drop(cols_to_drop, axis=1, inplace=True) # keep the changes with inplace\n",
    "df.shape # observe the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now select the columns we would like to keep and then deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the following variables\n",
    "df_one = df[['id', 'host_since', 'host_is_superhost', 'host_identity_verified', 'city', 'latitude', 'longitude',\n",
    "             'is_location_exact', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price',\n",
    "             'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', 'maximum_nights', 'availability_30', 'number_of_reviews',\n",
    "             'review_scores_rating', 'review_scores_cleanliness', 'instant_bookable', 'cancellation_policy', 'calculated_host_listings_count']].copy()\n",
    "\n",
    "df_one.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we will be focusing on what we will subjectively consider an affordable place to stay at. So we will choose a price range from 25 to 400 per night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mask = ((df['price'] > 25) & (df['price'] < 400))\n",
    "price_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = df_one[price_mask].copy()\n",
    "df_two.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the rest of the missing values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two.isna().sum() / df_two.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with the variables with more than 20% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two[['cleaning_fee', 'review_scores_rating', 'review_scores_cleanliness']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the median to fill in the `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots_missing = ['cleaning_fee', 'review_scores_rating', 'review_scores_cleanliness']\n",
    "\n",
    "for col in lots_missing:\n",
    "    df_two[col] = df_two[col].fillna(df_two[col].median())\n",
    "\n",
    "df_two[lots_missing].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the memory usage at the very bottom\n",
    "df_two.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two.isna().sum() / df_two.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is essentially scraped data and it would be a bit complicated to figure out the actual date a host started hosting guests through Airbnb, we will use forward fill to fill in the few missing dates left in the `host_since` variable. Remember, how we deal with missing values will depend on many nuances and also in our intuition as data analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two['host_since'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also convert the categorical variables into the pandas `category` data type since it will represent the categories as integers with labels on top of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = ['host_is_superhost', 'host_identity_verified', 'is_location_exact', \n",
    "             'room_type', 'instant_bookable', 'cancellation_policy']\n",
    "\n",
    "for col in cols_list: # iterate over the column names\n",
    "    df_two[col].astype('category', copy=True) # change the column with that title to a category data type\n",
    "    if df_two[col].isna().any(): # if there are any missing values use the forward fill method on them\n",
    "        df_two[col].fillna(method='ffill', axis=0, inplace=True) # keep the changes in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with some of the essentials of a place, we will pick the option that makes most sence. In this case, the median seems to be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two[['bathrooms', 'bedrooms', 'bedrooms']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essentials = ['bathrooms', 'bedrooms', 'beds']\n",
    "for col in essentials:\n",
    "    df_two[col].fillna(df_two[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a variable that resembles the true price of a stay by multiplying the price per night by the minimum number of nights required per stay at a given place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two['min_price_stay'] = (df_two['price'] * df_two['minimum_nights'])\n",
    "df_two['min_price_stay'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some dates variables to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two['month'] = df_two['host_since'].dt.month\n",
    "df_two['year'] = df_two['host_since'].dt.year\n",
    "df_two['week'] = df_two['host_since'].dt.week\n",
    "df_two['weekday'] = df_two['host_since'].dt.weekday\n",
    "df_two['quarter'] = df_two['host_since'].dt.quarter\n",
    "df_two['day_of_week'] = df_two['host_since'].dt.day_name()\n",
    "df_two['week_or_end'] = df_two['weekday'].apply(lambda x: 'weekend' if x >= 5 else 'week_day')\n",
    "df_two.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the missing values one last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two.isna().sum() / df_two.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission accomplished, we now have a clean dataset to work with. Let's proceed to save it so that we don't have to repeat the process in case we need to turn off our computers, come back, and pick up where we left.\n",
    "\n",
    "It is not a must to reset the index, and you specially don't want to do it if it is the only identifier you have in common between the raw data and the clean one, but since we have an `id` column, the index doesn't add or subtract much to our dataset. Hence why we will reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_two.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save clean data for later use\n",
    "\n",
    "df_ready.to_csv('mel_airbnb_ready.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Describing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cool_stats](https://miro.medium.com/max/2416/1*x562RQ21PKPV4BCdt2TkJg.gif)\n",
    "**Source:** https://github.com/jwilber/roughViz\n",
    "\n",
    "**What are descriptive statistics?**\n",
    "\n",
    "Descriptive Statistics is the process of describing and presenting your data. This is usually done through tables, visualisations and written descriptions of data. For example, if you conduct a survey to find out how much people like a particular brand, you will want to report the number of people that took the survey (the count), the average, minimum, and maximum age or even the median income of every respondant. With these data alone, we could move onto to making more informed and important decisions.\n",
    "\n",
    "In the previous survey example, every survey taker would be a row, and every question in that survey would form the columns (see table below). Income, age, and favourite brand, would all represent different arrays or vectors of information in a two-dimensional matrix or dataset.\n",
    "\n",
    "|    Name   |   Income  |  Age  | Favourite Brand |\n",
    "|-----------|-----------|-------|-----------------|\n",
    "|  Abraham  |   70,000  |   25  |   Nike          |\n",
    "|  Lisa     |  110,000  |   31  |   Apple         |\n",
    "|  Mark     |   80,000  |   24  |   Garmin        |\n",
    "|  Penelope |   55,000  |   49  |   Microsoft     |\n",
    "|  Michaela |   74,000  |   51  |   ZARA          |\n",
    "|  Nicholas |   61,000  |   36  |   Adiddas       |\n",
    "\n",
    "Every one of these arrays can represent a categrorical or quantitative variable. Since data are often messy and even unstructured, we might also find other data structures, or free text (long strings) as the elements of a table like the one above. For the most part though, we will focus on the two types of variables we will often see, quantitative and categorical variables to describe our dataset.\n",
    "\n",
    "Descriptive statistics are often shown through tables and visualisations. For example, imagine you'd like to know the average life expectancy of people at age 60 in a specific group of countries. That visualisation would look like the one below.\n",
    "\n",
    "![life_exp](pictures/live_exp.png)\n",
    "\n",
    "__So what should we look for when we want to describe data?__ We want to look for information that gives us facts about the data, such as the most common value of a set of characteristics, how far is a given value in variable from the average value of that same variable (e.g. how far is the age of a teacher selected at random from the average age of all teachers in Sydney). These two kinds of values, the average of a set and the variability of each value in a set, are part of what is known as measures of central tendency and measures of variability, respectively.\n",
    "\n",
    "__Measures of Central tendency__, also classified as summary statistics, are calculations that help us describe any data point in our dataset in relation to the most common values. The most common determinants of central tendency are the mean, median, and mode.\n",
    "\n",
    "__Measures of Variability__ tell us how spread-out our data is. Think about this as how much variation there is, whether up or down, in the income of your closest friends to that of the income of the average Australian. That distance from the income of your friends to the most common income (the average of all) gives us insights as to how much income variability there is in the entire population of Australia. In this instance, your friends would be consider a sample of the population.\n",
    "\n",
    "The most common measures of variability are the range, variance and standard deviation.\n",
    "\n",
    "**How do we describe categories and quantities?**\n",
    "\n",
    "Categorical variables can be shown using frequency tables and visualisations where each category of the variable is given a number representing the number of times that category was found in the dataset. For example, a question in a survey containing answer choices ranging from Strongly Agree to Strongly Disagree will have 5 categories.\n",
    "\n",
    "Quantitative variables, on the other hand, can be summarised with averages, percentiles, scatterplots, etc., and tend to provide different perspectives and insights than qualitative variables.\n",
    "\n",
    "The most useful set of facts we will need will often come from a combination of both, qualitative and quantitative variables. Descriptive Statistics are specially useful when we have a lot of data and want to convey relevant information fast.\n",
    "\n",
    "Let's go over each one of the most important descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mel_airbnb_ready.csv', parse_dates=['host_since'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will need to know how many observations (also known as the n count) are in our dataset and that is exactly what `len()` gives us. We pass an array of values through the function and returns the number of all values in that array. `len()` also takes in strings and gives us back the total number of elements inside a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count specific categories visually with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='room_type', data=df, kind='count')\n",
    "plt.xlabel(\"Room Type\")\n",
    "plt.title(\"Count per room type offered in Melbourne\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we call the mean is actually the arithmetic mean. This is the sum of all the values in a set or array, divided by the amount of numbers in such array. While zeros are always counted in the arithmetic mean, in Python, empty values or `NaN`s are never counted towards that or any other operation.\n",
    "\n",
    "$\\overline{X}=\\dfrac{1}{n}\\sum x_{i}$\n",
    "\n",
    "In the folmula above, \n",
    "- $\\overline{X}$ stands for the mean\n",
    "- `n` is the lenght of the array, vector, set, or list\n",
    "- $\\dfrac{1}{n}$ means we will divide everything by the lenght\n",
    "- The greek letter $\\sum$ denotes the sumation of all proceding values\n",
    "- $x_{i}$ means every `x` value starting from `i`. In other words, every element of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pandas fload option to 5 decimal places helps us get rid of scientific notation\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the mean of all quantitative variables in a dataframe with the method `.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same works with np methods\n",
    "np.mean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the mean ourselves with plain Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['price']) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median will order an array of numbers from lowest to highest and select the number in the middle. This means that the median is effectively the 50th percentile of any array.\n",
    "\n",
    "If the array has an even amount of numbers, it will return the average of the middle two numbers. If the arrays has an odd amount of numbers, it will return the one in the middle.\n",
    "\n",
    "Just like with the `.mean()` method, we can use the `.median()` method on our dataframe and it will return the a median value for all numerical variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that our array has an odd number of elements, so if we want to get the number at the 50th percentile of our variable `price`, we could pass in a slice that selects that same middle number. We need to use a floor division because any regular division in Python returns a float and we can't pass floats to a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd array\n",
    "sorted(df['price'])[len(df) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also divide normally and surround the operation with `int()` to make sure we get an integer for our slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd array again\n",
    "sorted(df['price'])[int(len(df) / 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually get the median of an even array, we need to do a bit more work. We need to\n",
    "\n",
    "- sort the array\n",
    "- create a slice for the middle number and subtract one\n",
    "- add the value on the previous step to another slice without subtracting 1\n",
    "- divide the result by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even array\n",
    "arr = np.arange(10)\n",
    "\n",
    "# sum the lower and higher slicers surrounding the median and divide by 2\n",
    "(sorted(arr)[len(arr) // 2 - 1] + sorted(arr)[len(arr) // 2]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mode is the most frequent number in an array of numbers. To get the mode we can pass the `.mode()` method to a series or, we take advantage of the library SciPy. (SciPy stands for scientific computing.) This library is a cousing of NumPy and it comes with many similar, and other new functionalities on top of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the mode function from scipy\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we import the `mode` function we can pass it to our dataframe and it will return a scipy object with two arrays, one for the values that appear the most, and another for the times it appears the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass in this structure to an `np.array()`, transpose it, and get an array of tuples with each element and its respective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array((mode(df))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentiles give us the value at a given location of the array. For example, `np.percentile(array, 25)` will return the number where 75% of the data is above of, and 25% of the data is below of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['min_price_stay'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['min_price_stay'], 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a particular percentile manually by multiplying percentages by the length of the array inside a slice on a sorted array. Think of this as masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['min_price_stay'])[int(0.75 * len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Range of a set is the difference between the maximum and minimum numbers of an array. Going back to our income example, if the highest-paid person in Australia made 2,000,000/year and the lowest-paid person made 500/year, then the range of that set would be the difference between the two, or 1,999,500. This is regardless of the lenght of the array. Range focuses on content.\n",
    "\n",
    "We can use `np.ptp()` to get the range of a numerical array or we can compute the range ourselves in several different ways with regular Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ptp(df['cleaning_fee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['cleaning_fee']) - min(df['cleaning_fee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['cleaning_fee'])[-1] - sorted(df['cleaning_fee'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance tells us how much variation to expect in an array of data ($x$) with at least 2 numbers. For example, imagine that we have two groups, each with 5 professional athletes (two arrays of 5 elements each). One group of soccer players and another group of tennis players. We first ask each group how much do they spend eating out each week. We then calculate the average of those amounts, and, to our surprise, we find out that both groups of athletes spend the same, 570/week eating out.\n",
    "\n",
    "Although the average is the same, the variation between both can be extremely large. If one array has `[570, 570, 570, 570, 570]` and the other has `[700, 380, 200, 650, 920]`, the variation of the first would be equal to `0` while the variation of the second would be much greater than that.\n",
    "\n",
    "Here is the mathematical formula for the variance. Where sum the squared difference of each data point of an array by mean of that array. We then divide the sum the the lenght of the array.\n",
    "\n",
    "$S^{2}=\\dfrac {1}{n-1}\\sum _{i}\\left( x_{i}-\\overline {x}\\right) ^{2}$\n",
    "\n",
    "In the folmula above, \n",
    "- $\\overline{X}$ stands for the mean\n",
    "- `n` is the lenght of the array, vector, set, or list\n",
    "- $\\dfrac{1}{n-1}$ means we will divide everything by the lenght\n",
    "- The greek letter $\\sum$ denotes sumation\n",
    "- $x_{i}$ means every `x` value starting from `i`. In other words, every element of the array\n",
    "- $S^{2}$ --> squared standard deviation of a set\n",
    "- $\\left( x_{i}-\\overline {x}\\right) ^{2}$ means the square difference\n",
    "\n",
    "Let's calculate the variance using different methods in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a method on the entire array\n",
    "df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the numpy method on a one dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(df['minimum_nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last option is to do it ourselved with by coding every step of the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([(x - df['price'].mean()) ** 2 for x in df['price']]) / (len(df['price']) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what just happened in the cell above.\n",
    "\n",
    "- We initialised a list comprehension\n",
    "- we subtracted the mean of the price column from every one its prices\n",
    "- we squared the result\n",
    "- sumed everything up\n",
    "- and finished by dividing the final product by the n count minus 1\n",
    "\n",
    "In statistics, the variance and the standard deviation of a sample al always divided by `n - 1`. This is to signify the understatement of the true parameter $\\overline {X}$ which we don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Standard Deviation measures the dispersion of data from its mean. Think of the dispertion as percentage blocks of data surrounding the average value (see the picture below).\n",
    "\n",
    "![std](https://sway.office.com/s/EfPj5fmDwSziDupy/images/iJQFxVhHL6o7Sq?quality=860&allowAnimation=false)  \n",
    "**Source:** https://sixsigmadsi.com/standard-deviation-measure-of-dispersion/\n",
    "\n",
    "Every data point in these blocks is said to be 1, 2, or 3 standard deviations from the average value. In addition, these blocks provide us with a percentage of expected values, meaning, if we were to ask how many of the data points in our set are 1 standard deviation below the mean, we would mention that X% of our data points would land on that quadrant.\n",
    "\n",
    "A more succint example would be, if crazy rainy days in Australia happened with a frequency of 1 standard deviation below the mean, and if this set of weather temperatures was normally distributed, we could say that we would expect crazy rainy weather about 34% of the time in a year. Of course, we could be even more specific about this.\n",
    "\n",
    "The important distinction to keep in mind is that sequential regions within a distribution can only be found evenly when the distribution is normally distributed.\n",
    "\n",
    "`np.std()` and `df.std()` will return the standard deviation of an array or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum & Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minimum & Maximum are the lowest and highest values in an array, respectively. These are useful when we have quantitative variables such as income, or house prices, but not when we have categorical variables such as gender or weekdays. For example, imagine having a variable called food temperature that is classified as hot, warm, or cold (with numerical equivalents of 1, 2, 3). Our functions MIN and MAX will not be very useful for this categorical variable since the distance of such categories would not carry much meaning.\n",
    "\n",
    "We can pass in min and max as methods or as a numpy functions to our dataframe and also to specific columns or Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort the array and select the first and last elements for the min and the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual min\n",
    "sorted(df['minimum_nights'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual max\n",
    "sorted(df['minimum_nights'])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Skewness of a distribution tells us how distorted the distribution of our variable is from the most common value or the peak of the curve.\n",
    "\n",
    "A rightly-skewed distribution such as the one below is said to be positively skewed. The opposite means  that a skewed mountain going the other way would be anegatively skewed distribution.\n",
    "\n",
    "Another important point to remember is that the mean of a positively-skewed distribution will be larger than the median, and the opposite is true for a negatively-skewed distribution. When skeweness is zero, there's no distortion in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['price'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3})\n",
    "plt.title(\"Density distribution for Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass the `.skew()` method to our dataframe and get a sense of how distorted all of our variables might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy has convenient skew formula in the stats library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning fee Skewness is %.2f\" % skew(df['min_price_stay']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis is a very useful statistic that tells us how much do the tails of the distribution of a variable differ from those of a normal distribution (e.g. a true bell-shaped curve). It tells us if we are dealing with extreme values.\n",
    "\n",
    "The kurtosis of a normal distribution is usually at the value 3. A much higher kurtosis than this means that we are dealing with outliers. On the other hand, a lower kurtosis means that the distribution we are dealing with will see less extreme values than those seen in a normaly distributed array with the same data.\n",
    "\n",
    "Our pandas dataframe has to methods to calculate the Kurtosis. `.kurt()` and `.kurtosis()`. They both return the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kurtosis() == df.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning fee Kurtosis %.2f\" % kurtosis(df['cleaning_fee']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data analysts we want to be able to determine how does one variable changes or moves in relation to another. We can do this visually using quantitative variables with scatter plots, or with some handy mathematical functions that we can either create ourselves, or used from libraries like numpy and scipy.\n",
    "\n",
    "**Correlation** is a measure of how strongly related two variables are. It gives us a way of quantifying the similarity, disimilarity, or lack-therof between variables. The value of the correlation between two variables goes from -1 to 1, where 1 means positively correlated, -1 means negatively correlated, and 0 means no correlation whatsoever. This value is derived by calculating the **Pearson Correlation Coefficient**.\n",
    "\n",
    "![corr](https://www.investopedia.com/thmb/PXAx5y_OS5z7n-Rn9m--QOC29rw=/1500x1000/filters:no_upscale():max_bytes(150000):strip_icc()/TC_3126228-how-to-calculate-the-correlation-coefficient-5aabeb313de423003610ee40.png)  \n",
    "**Source:** [Investopedia](https://www.investopedia.com/ask/answers/032515/what-does-it-mean-if-correlation-coefficient-positive-negative-or-zero.asp)\n",
    "\n",
    "The correlation between two or more variables can also be visually inspected through visualisations such as scatter plots and they can also be directly computed by hand or using several functions in Python. The mathematical formula is:\n",
    "\n",
    "$r_{xy}=\\dfrac {\\sum \\left( x_{i}-\\overline {x}\\right) \\left( y_{i}-\\overline {y}\\right) }{\\sqrt {\\sum \\left( x_{i}-\\overline {x}\\right) ^{2}\\sum \\left( y_{i}-\\overline {y}\\right) ^{2}}}$\n",
    "\n",
    "\n",
    "Where\n",
    "\n",
    "- $r_{xy}$ is the relationship between the variables X and Y\n",
    "- $x_{i}$ is every element in array X\n",
    "- $y_{i}$ is every element in array Y\n",
    "- $\\overline {x}$ is the mean of array X\n",
    "- $\\overline {y}$ is the mean of array Y\n",
    "\n",
    "Essentially, we are dividing the covariance between the product of the standard deviations of each array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some columns for our prices\n",
    "prices = df['price']\n",
    "clean_fee = df['cleaning_fee']\n",
    "\n",
    "mean_price = prices.mean()\n",
    "mean_clean = clean_fee.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's sum up the product of the differences between the elements in each array and their mean\n",
    "arrays_diff_sum = sum((prices - mean_price) * (clean_fee - mean_clean))\n",
    "arrays_diff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now compute the square root of the sum of the squared difference between each of the \n",
    "# elements in an array and its mean\n",
    "diff_sqrt = np.sqrt(sum((prices - mean_price)**2) * sum((clean_fee - mean_clean)**2))\n",
    "diff_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_xy = arrays_diff_sum / diff_sqrt\n",
    "print(\"The correlation between regular prices per stay and the cleaning fees is: %.3f\" % r_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can conveniently call the pandas method .corr with two Series\n",
    "print(\"Correlation between prices per stay and the cleaning fees is: %.3f\" % prices.corr(clean_fee))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pearsonr()` function from scipy stats will also give you the pearson correlation plus the p-value of the arrays. The P-value is the probability that you would have found the current result if the correlation coefficient were in fact zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(prices, clean_fee)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='min_price_stay', y='cleaning_fee', data=df)\n",
    "plt.title(\"Minimum Price per Stay VS Cleaning Fees\")\n",
    "plt.xlim((-50, 50000))\n",
    "plt.ylim((-15, 600))\n",
    "plt.xlabel(\"Price per Stay\")\n",
    "plt.ylabel(\"Cleaning Fee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance:** this statistic tells us how two variables vary together. That is, how does the variation of one variable relate to the variation in another.\n",
    "\n",
    "We can calculate the covariance of two arrays X and Y by first computing the distance between the mean and each of the values within the arrays. `dx` below represents the distance of each value `x` from its mean. The same is true for the `dy`.\n",
    "\n",
    "$dx = x_{i} - \\overline {x}$  \n",
    "$dy = y_{i} - \\overline {y}$\n",
    "\n",
    "\n",
    "Lastly, we multiply each distance from both arrays, add the resulting values, and then divide by the lenght of one of the arrays (which have to be both of the same).\n",
    "\n",
    "$cov\\left( X,Y\\right) =\\dfrac {1}{n}\\sum dx_{i}dy_{i}$\n",
    "\n",
    "If both arrays variate in the same direction, the result will be positive. It their variations are completely unrelated, we are likely to get a 0. If both arrays variate negatively, the result will be negative. This is also visible when the a corresponding value in each array is below or above the mean. In this instance, the latter would be positive and former negative.\n",
    "\n",
    "Both numbers tend to scale with each other. A large positive covariance means that if x is large then y is large as well, and when x is small then y is small as well. If the covariance between arrays is 0, there is no relationship whatsoever between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.dot takes the products of two elements and sums them up\n",
    "np.dot((prices - prices.mean()), (clean_fee - clean_fee.mean())) / len(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((prices - prices.mean()) * (clean_fee - clean_fee.mean())) / len(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Pearson Correlation only measures whether there is a linear relationship between the variables in question. This mean, that the relationship could be other than linear, hence, no relationship in the Pearson's test doesn't mean the isn't a relationship at all.\n",
    "\n",
    "One of the flaws of pearson correlation is that it does not deal well with outliers. There are other methods available for determining non-linear relationships that do deal with outliers, but we won't cover those in this lesson. They are the Spearman Rank Correlation test and the Kendall Correlation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-tabulating and Visualising Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to use descriptive statistics to gain deeper insights from a particular dataset is by cross-tabulating its variables. This technique allows us to aggregate and observe data based on distinct variables sitting on top of one another (columns) or standing next to each other (rows).\n",
    "\n",
    "You have already seen the cross-tabulation method in lesson's 3 and 4 but let's dig a bit deeper into how to use these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the comparison between prices charged between super hosts and non superhosts throughout the days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_hosts = df.pivot_table(\n",
    "        aggfunc=['mean'],\n",
    "        values=['price'],\n",
    "        index=['day_of_week'],\n",
    "        columns=['host_is_superhost']\n",
    ")\n",
    "super_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_hosts.plot(kind='bar', title='Average Price per Night of Super vs Non Super Hosts')\n",
    "plt.legend(['False', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would also be interesting to see whether there is a big gap between the distribution of prices across rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('notebook')\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.violinplot(x=\"week_or_end\", y=\"price\", hue=\"room_type\", data=df, palette=\"Pastel1\")\n",
    "plt.title(\"Price variation between weekdays and weekends by Room Type\")\n",
    "plt.xlabel(\"Stage of the Week\")\n",
    "plt.ylabel(\"Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand groups better, we cam create a multi-index object using the method `.grouby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_rooms = df.groupby(['room_type', 'cancellation_policy'])\n",
    "group_rooms['min_price_stay'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon first inspection it seems as if the most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_super_host = df.pivot_table(\n",
    "    index='cancellation_policy',\n",
    "    values='min_price_stay',\n",
    "    columns='host_is_superhost',\n",
    "    aggfunc='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_super_host['diff'] = cancel_super_host['t'] - cancel_super_host['f']\n",
    "cancel_super_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_price = df.pivot_table(index='city', \n",
    "                                values='price', \n",
    "                                columns='host_identity_verified', \n",
    "                                aggfunc=['count', 'mean', 'var'])\n",
    "identity_price.loc['Banyule':'Melbourne', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create a pivot table with at least one categorical variable and two quantitaive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create a plot from a pivot table with a multi-index based on categories and the price as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create a pivot table with 4 aggregation functions, a column, and 2 indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered the basics of Statistics in this lesson. We touched on:\n",
    "\n",
    "- The three main branches of statistics: collecting, describing and inferring from data\n",
    "- Different approaches for collecting data through sampling, observation and experimentation\n",
    "- We also covered the main differences between structured and unstructured data\n",
    "- In-depth descriptive statistics and how to calculate these with plain Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herne, H., & Huff, D. (1973). _How to Lie with Statistics_. Applied Statistics, 22(3), 401. doi: 10.2307/2346789\n",
    "\n",
    "Downey, Allen B. _Think Stats: Exploratory Data Analysis in Python_. Green Tea Press, 2014.\n",
    "\n",
    "Lock, Robin H., et al. _Statistics: Unlocking the Power of Data_. John Wiley & Sons, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would really appreciate it if you could please provide us with your feedback from this session by filling a couple of question.\n",
    "\n",
    "> ## [Survey](https://docs.google.com/forms/d/e/1FAIpQLSda4GNkdIgBNvLMcTVAITQLF7rPOI25tkmQIljl15SnkhEgsw/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
